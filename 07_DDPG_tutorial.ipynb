{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba0632",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG), Actor-Critic agents and How to Define Output layers\n",
    "\n",
    "Deep Deterministic Policy Gracient (DDPG) is an Actor-Critic algorithm that extend DQN to create an agent for solving problems with continuous actions. This agent consist of two neural networks: 1) the Actor network receives states and propose actions and 2) the Critic network recieves the states and the actions to calculate the advantage values A(s, a).\n",
    "\n",
    "Aditionally, we will see how to define the output layers of each network when we use keras, since we let the library calculate it automatically until now.\n",
    "\n",
    "By the end of this tutorial you will know how to use DDPG agents and how to define the properties of the output layers of your networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import ddpg_agent\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from RL_Problem.base.ActorCritic import ddpg_problem\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663b605",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture\n",
    "\n",
    "Here, we define the neural networks using keras. We created two functions, one for creating the actor network and another for the critic network. When creating the critic network for DDPG we have to take into acount that this network need two different inputs: 1) the state, as usual in other agents. In this exmaple we stack 5 time steps and use an LSTM as first layer. And 2) the actions, which consist of a array of 2 values because we have two actions in the selected environment.\n",
    "\n",
    "Notice that we also define the output layers, doing this allows the user to define for example their prefered activation, add a normalization in the output or even use a Lambda function to sample from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc20816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_custom_model(input_shape):\n",
    "    lstm = LSTM(32, activation='tanh', input_shape=input_shape, name='lstm_c')\n",
    "    dense_1 = Dense(256, activation='relu')\n",
    "    dense_2 = Dense(128, activation='relu')\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(2, activation='tanh')\n",
    "    \n",
    "    def model():\n",
    "        model = tf.keras.models.Sequential([lstm, dense_1, dense_2, output])\n",
    "        return model\n",
    "    return model()\n",
    "\n",
    "def critic_custom_model(input_shape, actor_net):\n",
    "    \n",
    "    lstm_s = LSTM(32, activation='tanh', input_shape=input_shape, name='lstm_state')\n",
    "    dense_s = Dense(256, activation='relu', name='dense_state')\n",
    "    \n",
    "    dense_a = Dense(128, activation='relu', input_shape=(actor_net.output.shape[1:]), name='dense_act')\n",
    "    \n",
    "    dense_c = Dense(128, activation='relu', name='dense_common')\n",
    "    output = Dense(1, activation='linear', name='output')\n",
    "    \n",
    "    def model():\n",
    "        \n",
    "        # state model\n",
    "        state_model = tf.keras.models.Sequential([lstm_s, dense_s])   \n",
    "        \n",
    "        # action model\n",
    "        act_model = tf.keras.models.Sequential([dense_a])\n",
    "        \n",
    "        # merge both models\n",
    "        merge = tf.keras.layers.Concatenate()([state_model.output, act_model.output])\n",
    "        merge = dense_c(merge)\n",
    "        \n",
    "        # Output layer\n",
    "        out = output(merge)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[state_model.input, act_model.input], outputs=out)\n",
    "        return model\n",
    "    return model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2049fe5",
   "metadata": {},
   "source": [
    "In the next cell, we define the neural network using dictionaries. As we have especified the output layers for Actor and Critic we have to set to True the \"define_custom_output_layer\" parameter to inform the agent of this fact. We also need to set to True the \"use_custom_network\" param.\n",
    "\n",
    "As we are using an Actor-Critic agent we need to set two parameters, one for each network: \"actor_custom_network\" and \"critic_custom_network\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eba5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.ddpg_net(use_custom_network=True,\n",
    "                                     actor_custom_network=actor_custom_model,\n",
    "                                     critic_custom_network=critic_custom_model,\n",
    "                                     define_custom_output_layer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea1351",
   "metadata": {},
   "source": [
    "## Define the RL Agent\n",
    "\n",
    "We define the Actor-Critic agent setting the next parameters:\n",
    "\n",
    "* actor_lr: learning rate for training the Actor neural network.\n",
    "* critic_lr: learning rate for training the Critic neural network.\n",
    "* batch_size: Size of the batches used for training the neural network.\n",
    "* epsilon: Determines the amount of exploration.\n",
    "* epsilon_decay: Decay factor of the epsilon. \n",
    "* esilon_min: minimun value epsilon can reach during the training procedure.\n",
    "* net_architecture: net architecture defined before.\n",
    "* n_stack: number of stacked timesteps to form the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ddpg_agent.Agent(actor_lr=1e-3,\n",
    "                         critic_lr=1e-3,\n",
    "                         batch_size=64,\n",
    "                         epsilon=0.5,\n",
    "                         epsilon_decay=0.9999,\n",
    "                         epsilon_min=0.15,\n",
    "                         net_architecture=net_architecture,\n",
    "                         n_stack=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f9d8c",
   "metadata": {},
   "source": [
    "## Define the Environment\n",
    "We chose the LunarLanderContinuous environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699247de",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"LunarLanderContinuous-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63c949",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "The RL problem is were the comunications between agent and environment are managed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = ddpg_problem.DDPGPRoblem(environment, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bb754",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "Here, we introduce a new parameter: \"max_step_epi\". It is used to limits the number of steps  of every episode. This is useful if we have an environment without a maximun limit of time steps or, as in this case, we want to reduce the maximun number steps fromm 1000 to 250 to force the agent to solve the problem faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ba32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(250, max_step_epi=200, render_after=150, skip_states=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ccd174",
   "metadata": {},
   "source": [
    "Run the agent in test mode to see the final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01715cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.test(render=False, n_iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ffd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc59a6",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "- We learned how to use DDPG agents.\n",
    "- We learned how to create custom network architectures with keras for Actor-Critic agents.\n",
    "- We learned how to define the output layers of owr networks to be able to set the desired activations or different functionalities.\n",
    "- We learned how to limit the episode time steps during training to avoid too long episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5718ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
