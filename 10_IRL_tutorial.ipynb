{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4184578e",
   "metadata": {},
   "source": [
    "# Training an agent through Inverse Reinforcement Learning\n",
    "\n",
    "This tutorial aims to show the use of Inverse Reinforcement Learning tools to train a RL agent via Imitation Learning. \n",
    "\n",
    "This library provides three Imitation Learning algorithms:\n",
    "\n",
    "## 1) Deep Inverse Reinforcement Learning (DeepIRL): \n",
    "\n",
    "\n",
    "It Consist of an implementation of \"Apprenticeship Learning algorithm from Pieter Abbeel and Andrew Y. Ng. 2004. Apprenticeship learning via inverse reinforcement learning. ICML '04.\"\n",
    "\n",
    "As an overview, this algorithm have two main entities with two adversarial task: 1) a RL agent generate actions that aims to be very similar to expert actions. 2) a discriminator tries to diferenciate what actions comes from a RL agent and what comes from an expert. This task produces as result a value that is used as reward to train the RL agent.\n",
    "\n",
    "This particular implememtation uses Deep Learning. For this purpose whe have replaced the classificator used fro the discriminator in the original work by a neural network. This algorithm is compatible with all Deep Reinforcement Learning agents in this library.\n",
    "\n",
    "## 2) Generative Adversarial Imitation Learning (GAIL)\n",
    "\n",
    "This is an implementation of \"HO, Jonathan; ERMON, Stefano. Generative adversarial imitation learning. Advances in neural formation processing systems, 2016, vol. 29, p. 4565-4573.\" \n",
    "\n",
    "This algorithm is very similar to DeepIRL but use the workflow of Trus Region Policy Optimization (TRPO) algorithm (this is another RL algorithm) to makes the process more efficient. Have two main entities: 1) a reinforcement learning agent that generates actions that aims to be very similar to the expert actions. 2) a discriminator neural network that tries to diferenciate what actions comes from a RL agent and what comes from an expert. This task produces as result a value that is used as reward to train the RL agent. \n",
    "\n",
    "This particular implementation uses Proximal Policy Optimizarion (PPO) instead of TRPO because PPO was created as a refined version of TRPO and both have the same workflow. (This means that GAIL is only compatible with PPO and no other RL agent can be used with it)\n",
    "\n",
    "## 3) Behavioral Cloning\n",
    "\n",
    "This algorithm consist of a supervised deep learning problem where a neural network is trained using a dataset of expert experiences which contains the states paired with actions. The neural network is trained using the states as inputs and the actions as labels.\n",
    "\n",
    "In this library we provide the tools to train the RL agents through behavioral cloining. This tolls also allows to pretrain a RL agent over labeled data and then make fine tuning with RL or IRL.\n",
    "\n",
    "## Expert Data\n",
    "\n",
    "All Imitation learning methods need a dataset of expert demonstrations. This dataset should contain the experiences on each time step. This experiences depending on the problem, may contain only the states of the states paired with actions. We also provides some utilities to store and load the exper datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3421561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.7.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 10:19:57.634324: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2021-12-10 10:19:57.802474: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2799925000 Hz\n",
      "2021-12-10 10:19:57.803944: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e0fc84da10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-12-10 10:19:57.803991: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-12-10 10:19:57.831715: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/serch/anaconda3/envs/capoir/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "from RL_Problem import rl_problem\n",
    "from IL_Problem.gail import GAIL\n",
    "from IL_Problem.deepirl import DeepIRL\n",
    "from RL_Agent import ppo_agent_continuous_parallel, dpg_agent_continuous\n",
    "from IL_Problem.base.utils.callbacks import load_expert_memories, Callbacks\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from RL_Agent.base.utils import agent_saver\n",
    "from RL_Agent.base.utils.networks import networks as rl_networks\n",
    "from IL_Problem.base.utils.networks import networks_dictionaries as il_networks\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36aed65",
   "metadata": {},
   "source": [
    "We are going to use the LunarLander environment from OpenAI Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98251f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"LunarLande-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e133025",
   "metadata": {},
   "source": [
    "We provide an expert demosntartions dataset in \"tutorials/tf_tutorials/expert_demonstrations/Expert_LunarLander.pkl\". This dataset was created runing an already trained DPG agent over the environment.\n",
    "\n",
    "Next, we provide the code we have used to generate the dataset with a DPG agent. If you already have a dataset, you do not need to run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"tutorials/tf_tutorials/expert_demonstrations/Expert_LunarLanderContinuous.pkl\"\n",
    "net_architecture = rl_networks.net_architecture(dense_layers=2,\n",
    "                                           n_neurons=[256, 256],\n",
    "                                           dense_activation=['relu', 'relu'])\n",
    "\n",
    "expert = dpg_agent_continuous.Agent(learning_rate=5e-4,\n",
    "                         batch_size=32,\n",
    "                         net_architecture=net_architecture)\n",
    "\n",
    "expert_problem = rl_problem.Problem(environment, expert)\n",
    "\n",
    "callback = Callbacks()\n",
    "\n",
    "# Comentar si ya se dispone de un fichero de experiencias como \"Expert_LunarLander.pkl\"\n",
    "print(\"Comienzo entrenamiento de un experto\")\n",
    "expert_problem.solve(1000, render=False, max_step_epi=250, render_after=980, skip_states=3)\n",
    "expert_problem.test(render=False, n_iter=400, callback=callback.remember_callback)\n",
    "\n",
    "callback.save_memories(exp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0e977",
   "metadata": {},
   "source": [
    "Define the agent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089cf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_custom_model(input_shape):\n",
    "    actor_model = Sequential()\n",
    "    actor_model.add(LSTM(16, input_shape=input_shape, activation='tanh'))\n",
    "    actor_model.add(Dense(256, input_shape=input_shape, activation='relu'))\n",
    "    actor_model.add(Dense(256, activation='relu'))\n",
    "    return actor_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb34c2f",
   "metadata": {},
   "source": [
    "Load the expert experiences.\n",
    "\n",
    "In \"IL_Problem.base.utils.callbacks.py\" we have some utilities for storing and solading expert experiences. Especifically, we use the function \"load_expert_memories\" which recieves three parameters: 1) \"path\", string with path to data. 2) \"load_action\", boolean to load or not the actions. We can performs IRL training the discriminator in differenciate only the states reached by an expert from the states reached by the agent or to differenciante the the state-action pairs from the expert and agent. 3) \"n_stack\" defines how many temporal steps will be stacked to form the state when using the discriminator. We can used stacket states for the agent but not for the discriminator or use it for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c0f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"tutorials/tf_tutorials/expert_demonstrations/Expert_LunarLanderContinuous.pkl\"\n",
    "\n",
    "use_expert_actions = True\n",
    "discriminator_stack = 3\n",
    "exp_memory = load_expert_memories(exp_path, load_action=use_expert_actions, n_stack=discriminator_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee41fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = rl_networks.ppo_net(use_custom_network=True,\n",
    "                                        actor_custom_network=lstm_custom_model,\n",
    "                                        critic_custom_network=lstm_custom_model\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60078c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent_continuous_parallel.Agent(actor_lr=1e-4,\n",
    "                                          critic_lr=1e-4,\n",
    "                                          batch_size=128,\n",
    "                                          epsilon=0.9,\n",
    "                                          epsilon_decay=0.97,\n",
    "                                          epsilon_min=0.15,\n",
    "                                          memory_size=1024,\n",
    "                                          net_architecture=net_architecture,\n",
    "                                          n_stack=discriminator_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d570d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_problem = rl_problem.Problem(environment, agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200bd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_custom_model(input_shape):\n",
    "    x_input = Input(shape=input_shape, name='disc_s_input')\n",
    "    x = Dense(128, activation='relu')(x_input)\n",
    "    x = Dense(128, input_shape=input_shape, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=x_input, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce94a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_net_architecture = il_networks.irl_discriminator_net(use_custom_network=True,\n",
    "                                                         common_custom_network=one_layer_custom_model,\n",
    "                                                         define_custom_output_layer=True,\n",
    "                                                         use_tf_custom_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9924159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serch/TFM/CAPOIRL-TF2/IL_Problem/base/il_problem_super.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.expert_traj = np.array([[np.array([self.preprocess(o) for o in x[0]]), x[1]] for x in expert_traj])\n"
     ]
    }
   ],
   "source": [
    "irl_problem = DeepIRL(rl_problem, exp_memory, lr_disc=1e-5, batch_size_disc=128, epochs_disc=2, val_split_disc=0.1,\n",
    "                      agent_collect_iter=10, agent_train_iter=25, n_stack_disc=discriminator_stack,\n",
    "                      net_architecture=irl_net_architecture, use_expert_actions=use_expert_actions, tensorboard_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f597b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento de agente con aprendizaje por imitación\n",
      "Test episode:  1 Epochs:  84  Reward: -99.2 Smooth Reward: -99.2  Epsilon: 0.9000\n",
      "Test episode:  2 Epochs:  67  Reward: -120.5 Smooth Reward: -109.9  Epsilon: 0.9000\n",
      "Test episode:  3 Epochs:  122  Reward: -81.9 Smooth Reward: -101.2  Epsilon: 0.9000\n",
      "Test episode:  4 Epochs:  69  Reward: -163.0 Smooth Reward: -122.5  Epsilon: 0.9000\n",
      "Test episode:  5 Epochs:  105  Reward: -155.6 Smooth Reward: -159.3  Epsilon: 0.9000\n",
      "Test episode:  6 Epochs:  65  Reward: -106.6 Smooth Reward: -131.1  Epsilon: 0.9000\n",
      "Test episode:  7 Epochs:  78  Reward: -93.9 Smooth Reward: -100.2  Epsilon: 0.9000\n",
      "Test episode:  8 Epochs:  99  Reward: -121.6 Smooth Reward: -107.7  Epsilon: 0.9000\n",
      "Test episode:  9 Epochs:  99  Reward: -114.0 Smooth Reward: -117.8  Epsilon: 0.9000\n",
      "Test episode:  10 Epochs:  135  Reward: -181.1 Smooth Reward: -147.6  Epsilon: 0.9000\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.2551 binary_accuracy 0.3445 val_loss  0.2525 val_binary_accuracy 0.4164\n",
      "epoch 2\t loss  0.2526 binary_accuracy 0.4089 val_loss  0.2505 val_binary_accuracy 0.4326\n",
      "Episode:  1 Epochs:  1024  Reward: 465.3 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Episode:  2 Epochs:  1024  Reward: 493.1 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Episode:  3 Epochs:  1024  Reward: 479.6 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Episode:  4 Epochs:  1024  Reward: 477.6 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Episode:  5 Epochs:  1024  Reward: 460.7 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Episode:  6 Epochs:  1024  Reward: 478.0 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Episode:  7 Epochs:  1024  Reward: 469.9 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Episode:  8 Epochs:  1024  Reward: 480.6 Smooth Reward: 475.6  Epsilon: 0.9000\n",
      "Actor loss 0.59657294 0\n",
      "Critic loss 1.1890357 0\n",
      "Episode:  9 Epochs:  1024  Reward: 469.5 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Episode:  10 Epochs:  1024  Reward: 474.8 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Episode:  11 Epochs:  1024  Reward: 478.9 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Episode:  12 Epochs:  1024  Reward: 482.0 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Episode:  13 Epochs:  1024  Reward: 469.4 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Episode:  14 Epochs:  1024  Reward: 480.7 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Episode:  15 Epochs:  1024  Reward: 486.6 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Episode:  16 Epochs:  1024  Reward: 475.5 Smooth Reward: 476.4  Epsilon: 0.8730\n",
      "Actor loss 0.117291234 1\n",
      "Critic loss 0.45836568 1\n",
      "Episode:  17 Epochs:  1024  Reward: 489.2 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Episode:  18 Epochs:  1024  Reward: 490.5 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Episode:  19 Epochs:  1024  Reward: 481.7 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Episode:  20 Epochs:  1024  Reward: 474.4 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Episode:  21 Epochs:  1024  Reward: 493.3 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Episode:  22 Epochs:  1024  Reward: 486.5 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Episode:  23 Epochs:  1024  Reward: 482.3 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Episode:  24 Epochs:  1024  Reward: 454.2 Smooth Reward: 479.3  Epsilon: 0.8468\n",
      "Actor loss 0.13690722 2\n",
      "Critic loss 0.4253501 2\n",
      "Episode:  25 Epochs:  1024  Reward: 479.7 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Episode:  26 Epochs:  1024  Reward: 492.4 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Episode:  27 Epochs:  1024  Reward: 469.2 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Episode:  28 Epochs:  1024  Reward: 495.5 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Episode:  29 Epochs:  1024  Reward: 489.9 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Episode:  30 Epochs:  1024  Reward: 483.6 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Episode:  31 Epochs:  1024  Reward: 483.2 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Episode:  32 Epochs:  1024  Reward: 475.4 Smooth Reward: 482.6  Epsilon: 0.8214\n",
      "Actor loss 0.007101978 3\n",
      "Critic loss 0.25800973 3\n",
      "Test episode:  1 Epochs:  80  Reward: -137.2 Smooth Reward: -137.2  Epsilon: 0.7968\n",
      "Test episode:  2 Epochs:  75  Reward: -142.3 Smooth Reward: -139.7  Epsilon: 0.7968\n",
      "Test episode:  3 Epochs:  68  Reward: -137.5 Smooth Reward: -139.9  Epsilon: 0.7968\n",
      "Test episode:  4 Epochs:  87  Reward: -144.8 Smooth Reward: -141.1  Epsilon: 0.7968\n",
      "Test episode:  5 Epochs:  71  Reward: -264.7 Smooth Reward: -204.7  Epsilon: 0.7968\n",
      "Test episode:  6 Epochs:  58  Reward: -156.3 Smooth Reward: -210.5  Epsilon: 0.7968\n",
      "Test episode:  7 Epochs:  82  Reward: -167.8 Smooth Reward: -162.1  Epsilon: 0.7968\n",
      "Test episode:  8 Epochs:  55  Reward: -87.9 Smooth Reward: -127.9  Epsilon: 0.7968\n",
      "Test episode:  9 Epochs:  79  Reward: -257.7 Smooth Reward: -172.8  Epsilon: 0.7968\n",
      "Test episode:  10 Epochs:  72  Reward: -163.7 Smooth Reward: -210.7  Epsilon: 0.7968\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.2475 binary_accuracy 0.4372 val_loss  0.2450 val_binary_accuracy 0.4637\n",
      "epoch 2\t loss  0.2434 binary_accuracy 0.4650 val_loss  0.2410 val_binary_accuracy 0.4856\n",
      "Episode:  33 Epochs:  1024  Reward: 467.3 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Episode:  34 Epochs:  1024  Reward: 465.2 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Episode:  35 Epochs:  1024  Reward: 480.3 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Episode:  36 Epochs:  1024  Reward: 455.6 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Episode:  37 Epochs:  1024  Reward: 475.7 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Episode:  38 Epochs:  1024  Reward: 476.2 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Episode:  39 Epochs:  1024  Reward: 473.9 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Episode:  40 Epochs:  1024  Reward: 456.5 Smooth Reward: 468.8  Epsilon: 0.7968\n",
      "Actor loss -0.07553803 4\n",
      "Critic loss 0.07922826 4\n",
      "Episode:  41 Epochs:  1024  Reward: 477.7 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Episode:  42 Epochs:  1024  Reward: 471.2 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Episode:  43 Epochs:  1024  Reward: 466.2 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Episode:  44 Epochs:  1024  Reward: 453.7 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Episode:  45 Epochs:  1024  Reward: 485.3 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Episode:  46 Epochs:  1024  Reward: 475.5 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Episode:  47 Epochs:  1024  Reward: 477.0 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Episode:  48 Epochs:  1024  Reward: 479.8 Smooth Reward: 471.1  Epsilon: 0.7729\n",
      "Actor loss 0.33239642 5\n",
      "Critic loss 0.38751882 5\n",
      "Episode:  49 Epochs:  1024  Reward: 473.9 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Episode:  50 Epochs:  1024  Reward: 470.1 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Episode:  51 Epochs:  1024  Reward: 466.0 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Episode:  52 Epochs:  1024  Reward: 453.9 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Episode:  53 Epochs:  1024  Reward: 467.1 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Episode:  54 Epochs:  1024  Reward: 466.6 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Episode:  55 Epochs:  1024  Reward: 456.8 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Episode:  56 Epochs:  1024  Reward: 468.1 Smooth Reward: 469.3  Epsilon: 0.7497\n",
      "Actor loss 0.2772067 6\n",
      "Critic loss 0.2230492 6\n",
      "Episode:  57 Epochs:  1024  Reward: 461.6 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Episode:  58 Epochs:  1024  Reward: 454.9 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Episode:  59 Epochs:  1024  Reward: 461.6 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Episode:  60 Epochs:  1024  Reward: 479.0 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Episode:  61 Epochs:  1024  Reward: 466.4 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Episode:  62 Epochs:  1024  Reward: 469.0 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Episode:  63 Epochs:  1024  Reward: 469.3 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Episode:  64 Epochs:  1024  Reward: 476.8 Smooth Reward: 466.3  Epsilon: 0.7272\n",
      "Actor loss 0.12964928 7\n",
      "Critic loss 0.10798434 7\n",
      "Test episode:  1 Epochs:  64  Reward: -234.3 Smooth Reward: -234.3  Epsilon: 0.7054\n",
      "Test episode:  2 Epochs:  58  Reward: -403.9 Smooth Reward: -319.1  Epsilon: 0.7054\n",
      "Test episode:  3 Epochs:  82  Reward: -296.4 Smooth Reward: -350.2  Epsilon: 0.7054\n",
      "Test episode:  4 Epochs:  53  Reward: -389.6 Smooth Reward: -343.0  Epsilon: 0.7054\n",
      "Test episode:  5 Epochs:  61  Reward: -491.0 Smooth Reward: -440.3  Epsilon: 0.7054\n",
      "Test episode:  6 Epochs:  66  Reward: -384.8 Smooth Reward: -437.9  Epsilon: 0.7054\n",
      "Test episode:  7 Epochs:  58  Reward: -189.2 Smooth Reward: -287.0  Epsilon: 0.7054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode:  8 Epochs:  86  Reward: -429.8 Smooth Reward: -309.5  Epsilon: 0.7054\n",
      "Test episode:  9 Epochs:  75  Reward: -403.8 Smooth Reward: -416.8  Epsilon: 0.7054\n",
      "Test episode:  10 Epochs:  86  Reward: -410.3 Smooth Reward: -407.0  Epsilon: 0.7054\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.2360 binary_accuracy 0.4901 val_loss  0.2332 val_binary_accuracy 0.5090\n",
      "epoch 2\t loss  0.2311 binary_accuracy 0.5111 val_loss  0.2282 val_binary_accuracy 0.5256\n",
      "Episode:  65 Epochs:  1024  Reward: 441.9 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Episode:  66 Epochs:  1024  Reward: 445.0 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Episode:  67 Epochs:  1024  Reward: 447.4 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Episode:  68 Epochs:  1024  Reward: 455.3 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Episode:  69 Epochs:  1024  Reward: 453.6 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Episode:  70 Epochs:  1024  Reward: 433.2 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Episode:  71 Epochs:  1024  Reward: 421.7 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Episode:  72 Epochs:  1024  Reward: 442.6 Smooth Reward: 442.6  Epsilon: 0.7054\n",
      "Actor loss -0.017579887 8\n",
      "Critic loss 0.052095488 8\n",
      "Episode:  73 Epochs:  1024  Reward: 416.0 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Episode:  74 Epochs:  1024  Reward: 419.4 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Episode:  75 Epochs:  1024  Reward: 438.3 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Episode:  76 Epochs:  1024  Reward: 417.0 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Episode:  77 Epochs:  1024  Reward: 407.7 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Episode:  78 Epochs:  1024  Reward: 422.4 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Episode:  79 Epochs:  1024  Reward: 437.5 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Episode:  80 Epochs:  1024  Reward: 439.3 Smooth Reward: 433.6  Epsilon: 0.6842\n",
      "Actor loss 0.09534254 9\n",
      "Critic loss 0.33945364 9\n",
      "Episode:  81 Epochs:  1024  Reward: 419.6 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Episode:  82 Epochs:  1024  Reward: 414.1 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Episode:  83 Epochs:  1024  Reward: 404.2 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Episode:  84 Epochs:  1024  Reward: 400.7 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Episode:  85 Epochs:  1024  Reward: 406.2 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Episode:  86 Epochs:  1024  Reward: 409.2 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Episode:  87 Epochs:  1024  Reward: 404.5 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Episode:  88 Epochs:  1024  Reward: 403.1 Smooth Reward: 416.2  Epsilon: 0.6637\n",
      "Actor loss 0.082358286 10\n",
      "Critic loss 0.029715259 10\n",
      "Episode:  89 Epochs:  1024  Reward: 401.6 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Episode:  90 Epochs:  1024  Reward: 393.3 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Episode:  91 Epochs:  1024  Reward: 403.4 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Episode:  92 Epochs:  1024  Reward: 408.8 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Episode:  93 Epochs:  1024  Reward: 409.2 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Episode:  94 Epochs:  1024  Reward: 405.7 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Episode:  95 Epochs:  1024  Reward: 399.4 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Episode:  96 Epochs:  1024  Reward: 406.8 Smooth Reward: 405.6  Epsilon: 0.6438\n",
      "Actor loss -0.045301303 11\n",
      "Critic loss 0.061053663 11\n",
      "Test episode:  1 Epochs:  50  Reward: -416.5 Smooth Reward: -416.5  Epsilon: 0.6245\n",
      "Test episode:  2 Epochs:  71  Reward: -672.0 Smooth Reward: -544.2  Epsilon: 0.6245\n",
      "Test episode:  3 Epochs:  75  Reward: -713.8 Smooth Reward: -692.9  Epsilon: 0.6245\n",
      "Test episode:  4 Epochs:  80  Reward: -674.5 Smooth Reward: -694.2  Epsilon: 0.6245\n",
      "Test episode:  5 Epochs:  77  Reward: -644.4 Smooth Reward: -659.4  Epsilon: 0.6245\n",
      "Test episode:  6 Epochs:  52  Reward: -383.2 Smooth Reward: -513.8  Epsilon: 0.6245\n",
      "Test episode:  7 Epochs:  54  Reward: -435.2 Smooth Reward: -409.2  Epsilon: 0.6245\n",
      "Test episode:  8 Epochs:  61  Reward: -462.0 Smooth Reward: -448.6  Epsilon: 0.6245\n",
      "Test episode:  9 Epochs:  74  Reward: -604.3 Smooth Reward: -533.1  Epsilon: 0.6245\n",
      "Test episode:  10 Epochs:  69  Reward: -580.1 Smooth Reward: -592.2  Epsilon: 0.6245\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.2200 binary_accuracy 0.5298 val_loss  0.2195 val_binary_accuracy 0.5476\n",
      "epoch 2\t loss  0.2142 binary_accuracy 0.5533 val_loss  0.2137 val_binary_accuracy 0.5678\n",
      "Episode:  97 Epochs:  1024  Reward: 358.1 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Episode:  98 Epochs:  1024  Reward: 382.1 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Episode:  99 Epochs:  1024  Reward: 353.9 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Episode:  100 Epochs:  1024  Reward: 379.9 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Episode:  101 Epochs:  1024  Reward: 364.7 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Episode:  102 Epochs:  1024  Reward: 370.2 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Episode:  103 Epochs:  1024  Reward: 381.1 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Episode:  104 Epochs:  1024  Reward: 361.1 Smooth Reward: 368.9  Epsilon: 0.6245\n",
      "Actor loss 0.011482553 12\n",
      "Critic loss 0.10457444 12\n",
      "Episode:  105 Epochs:  1024  Reward: 362.2 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Episode:  106 Epochs:  1024  Reward: 364.3 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Episode:  107 Epochs:  1024  Reward: 347.5 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Episode:  108 Epochs:  1024  Reward: 370.9 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Episode:  109 Epochs:  1024  Reward: 369.6 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Episode:  110 Epochs:  1024  Reward: 364.9 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Episode:  111 Epochs:  1024  Reward: 355.4 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Episode:  112 Epochs:  1024  Reward: 366.2 Smooth Reward: 365.8  Epsilon: 0.6057\n",
      "Actor loss -0.037561893 13\n",
      "Critic loss 0.16071653 13\n",
      "Episode:  113 Epochs:  1024  Reward: 367.2 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Episode:  114 Epochs:  1024  Reward: 361.1 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Episode:  115 Epochs:  1024  Reward: 371.8 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Episode:  116 Epochs:  1024  Reward: 375.5 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Episode:  117 Epochs:  1024  Reward: 358.0 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Episode:  118 Epochs:  1024  Reward: 363.4 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Episode:  119 Epochs:  1024  Reward: 370.3 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Episode:  120 Epochs:  1024  Reward: 371.0 Smooth Reward: 365.0  Epsilon: 0.5876\n",
      "Actor loss -0.06607857 14\n",
      "Critic loss 0.059331372 14\n",
      "Episode:  121 Epochs:  1024  Reward: 366.9 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Episode:  122 Epochs:  1024  Reward: 358.9 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Episode:  123 Epochs:  1024  Reward: 360.3 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Episode:  124 Epochs:  1024  Reward: 354.0 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Episode:  125 Epochs:  1024  Reward: 358.9 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Episode:  126 Epochs:  1024  Reward: 348.1 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Episode:  127 Epochs:  1024  Reward: 360.1 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Episode:  128 Epochs:  1024  Reward: 358.1 Smooth Reward: 362.7  Epsilon: 0.5699\n",
      "Actor loss -0.05033308 15\n",
      "Critic loss 0.07217985 15\n",
      "Test episode:  1 Epochs:  84  Reward: -524.3 Smooth Reward: -524.3  Epsilon: 0.5528\n",
      "Test episode:  2 Epochs:  76  Reward: -579.0 Smooth Reward: -551.7  Epsilon: 0.5528\n",
      "Test episode:  3 Epochs:  80  Reward: -511.5 Smooth Reward: -545.3  Epsilon: 0.5528\n",
      "Test episode:  4 Epochs:  56  Reward: -489.9 Smooth Reward: -500.7  Epsilon: 0.5528\n",
      "Test episode:  5 Epochs:  86  Reward: -705.9 Smooth Reward: -597.9  Epsilon: 0.5528\n",
      "Test episode:  6 Epochs:  76  Reward: -565.3 Smooth Reward: -635.6  Epsilon: 0.5528\n",
      "Test episode:  7 Epochs:  83  Reward: -451.1 Smooth Reward: -508.2  Epsilon: 0.5528\n",
      "Test episode:  8 Epochs:  65  Reward: -591.2 Smooth Reward: -521.1  Epsilon: 0.5528\n",
      "Test episode:  9 Epochs:  57  Reward: -434.3 Smooth Reward: -512.7  Epsilon: 0.5528\n",
      "Test episode:  10 Epochs:  58  Reward: -461.5 Smooth Reward: -447.9  Epsilon: 0.5528\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.2041 binary_accuracy 0.5760 val_loss  0.1979 val_binary_accuracy 0.5926\n",
      "epoch 2\t loss  0.1973 binary_accuracy 0.6024 val_loss  0.1914 val_binary_accuracy 0.6173\n",
      "Episode:  129 Epochs:  1024  Reward: 324.8 Smooth Reward: 323.0  Epsilon: 0.5528\n",
      "Episode:  130 Epochs:  1024  Reward: 317.4 Smooth Reward: 323.0  Epsilon: 0.5528\n",
      "Episode:  131 Epochs:  1024  Reward: 329.9 Smooth Reward: 323.0  Epsilon: 0.5528\n",
      "Episode:  132 Epochs:  1024  Reward: 321.6 Smooth Reward: 323.0  Epsilon: 0.5528\n",
      "Episode:  133 Epochs:  1024  Reward: 322.0 Smooth Reward: 323.0  Epsilon: 0.5528\n",
      "Episode:  134 Epochs:  1024  Reward: 327.2 Smooth Reward: 323.0  Epsilon: 0.5528\n",
      "Episode:  135 Epochs:  1024  Reward: 314.8 Smooth Reward: 323.0  Epsilon: 0.5528\n",
      "Episode:  136 Epochs:  1024  Reward: 326.5 Smooth Reward: 323.0  Epsilon: 0.5528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss -0.1258593 16\n",
      "Critic loss 0.046334863 16\n",
      "Episode:  137 Epochs:  1024  Reward: 319.7 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Episode:  138 Epochs:  1024  Reward: 324.1 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Episode:  139 Epochs:  1024  Reward: 316.3 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Episode:  140 Epochs:  1024  Reward: 329.1 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Episode:  141 Epochs:  1024  Reward: 323.3 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Episode:  142 Epochs:  1024  Reward: 321.9 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Episode:  143 Epochs:  1024  Reward: 327.4 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Episode:  144 Epochs:  1024  Reward: 324.7 Smooth Reward: 323.2  Epsilon: 0.5362\n",
      "Actor loss 0.019834746 17\n",
      "Critic loss 0.08491403 17\n",
      "Episode:  145 Epochs:  1024  Reward: 311.2 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Episode:  146 Epochs:  1024  Reward: 328.8 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Episode:  147 Epochs:  1024  Reward: 309.5 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Episode:  148 Epochs:  1024  Reward: 317.9 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Episode:  149 Epochs:  1024  Reward: 314.9 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Episode:  150 Epochs:  1024  Reward: 325.6 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Episode:  151 Epochs:  1024  Reward: 332.3 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Episode:  152 Epochs:  1024  Reward: 320.3 Smooth Reward: 321.7  Epsilon: 0.5202\n",
      "Actor loss 0.20537934 18\n",
      "Critic loss 0.037261292 18\n",
      "Episode:  153 Epochs:  1024  Reward: 305.9 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Episode:  154 Epochs:  1024  Reward: 321.3 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Episode:  155 Epochs:  1024  Reward: 327.3 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Episode:  156 Epochs:  1024  Reward: 309.6 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Episode:  157 Epochs:  1024  Reward: 318.2 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Episode:  158 Epochs:  1024  Reward: 314.6 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Episode:  159 Epochs:  1024  Reward: 321.5 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Episode:  160 Epochs:  1024  Reward: 330.2 Smooth Reward: 319.3  Epsilon: 0.5046\n",
      "Actor loss 0.073825374 19\n",
      "Critic loss 0.3011742 19\n",
      "Test episode:  1 Epochs:  80  Reward: -613.9 Smooth Reward: -613.9  Epsilon: 0.4894\n",
      "Test episode:  2 Epochs:  65  Reward: -626.4 Smooth Reward: -620.1  Epsilon: 0.4894\n",
      "Test episode:  3 Epochs:  84  Reward: -474.3 Smooth Reward: -550.3  Epsilon: 0.4894\n",
      "Test episode:  4 Epochs:  52  Reward: -462.5 Smooth Reward: -468.4  Epsilon: 0.4894\n",
      "Test episode:  5 Epochs:  77  Reward: -483.9 Smooth Reward: -473.2  Epsilon: 0.4894\n",
      "Test episode:  6 Epochs:  56  Reward: -458.5 Smooth Reward: -471.2  Epsilon: 0.4894\n",
      "Test episode:  7 Epochs:  64  Reward: -495.8 Smooth Reward: -477.1  Epsilon: 0.4894\n",
      "Test episode:  8 Epochs:  88  Reward: -340.6 Smooth Reward: -418.2  Epsilon: 0.4894\n",
      "Test episode:  9 Epochs:  72  Reward: -541.0 Smooth Reward: -440.8  Epsilon: 0.4894\n",
      "Test episode:  10 Epochs:  86  Reward: -565.8 Smooth Reward: -553.4  Epsilon: 0.4894\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.1889 binary_accuracy 0.6284 val_loss  0.1903 val_binary_accuracy 0.6431\n",
      "epoch 2\t loss  0.1820 binary_accuracy 0.6528 val_loss  0.1842 val_binary_accuracy 0.6658\n",
      "Episode:  161 Epochs:  1024  Reward: 287.5 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Episode:  162 Epochs:  1024  Reward: 280.8 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Episode:  163 Epochs:  1024  Reward: 294.1 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Episode:  164 Epochs:  1024  Reward: 294.3 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Episode:  165 Epochs:  1024  Reward: 314.4 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Episode:  166 Epochs:  1024  Reward: 294.5 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Episode:  167 Epochs:  1024  Reward: 290.6 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Episode:  168 Epochs:  1024  Reward: 297.4 Smooth Reward: 294.2  Epsilon: 0.4894\n",
      "Actor loss -0.09999787 20\n",
      "Critic loss 0.020598281 20\n",
      "Episode:  169 Epochs:  1024  Reward: 286.6 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Episode:  170 Epochs:  1024  Reward: 292.3 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Episode:  171 Epochs:  1024  Reward: 309.5 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Episode:  172 Epochs:  1024  Reward: 284.1 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Episode:  173 Epochs:  1024  Reward: 292.5 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Episode:  174 Epochs:  1024  Reward: 306.7 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Episode:  175 Epochs:  1024  Reward: 297.6 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Episode:  176 Epochs:  1024  Reward: 298.0 Smooth Reward: 295.1  Epsilon: 0.4747\n",
      "Actor loss 0.09602028 21\n",
      "Critic loss 0.008925226 21\n",
      "Episode:  177 Epochs:  1024  Reward: 286.7 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Episode:  178 Epochs:  1024  Reward: 287.2 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Episode:  179 Epochs:  1024  Reward: 285.9 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Episode:  180 Epochs:  1024  Reward: 293.3 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Episode:  181 Epochs:  1024  Reward: 295.7 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Episode:  182 Epochs:  1024  Reward: 288.9 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Episode:  183 Epochs:  1024  Reward: 300.7 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Episode:  184 Epochs:  1024  Reward: 297.5 Smooth Reward: 293.9  Epsilon: 0.4605\n",
      "Actor loss 0.08491405 22\n",
      "Critic loss 0.069142975 22\n",
      "Episode:  185 Epochs:  1024  Reward: 289.1 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Episode:  186 Epochs:  1024  Reward: 284.5 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Episode:  187 Epochs:  1024  Reward: 275.1 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Episode:  188 Epochs:  1024  Reward: 284.3 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Episode:  189 Epochs:  1024  Reward: 285.7 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Episode:  190 Epochs:  1024  Reward: 278.8 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Episode:  191 Epochs:  1024  Reward: 288.8 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Episode:  192 Epochs:  1024  Reward: 275.4 Smooth Reward: 287.3  Epsilon: 0.4467\n",
      "Actor loss 0.039261855 23\n",
      "Critic loss 0.010559619 23\n",
      "Test episode:  1 Epochs:  64  Reward: -639.1 Smooth Reward: -639.1  Epsilon: 0.4333\n",
      "Test episode:  2 Epochs:  79  Reward: -640.5 Smooth Reward: -639.8  Epsilon: 0.4333\n",
      "Test episode:  3 Epochs:  74  Reward: -468.4 Smooth Reward: -554.4  Epsilon: 0.4333\n",
      "Test episode:  4 Epochs:  86  Reward: -1007.6 Smooth Reward: -738.0  Epsilon: 0.4333\n",
      "Test episode:  5 Epochs:  51  Reward: -326.0 Smooth Reward: -666.8  Epsilon: 0.4333\n",
      "Test episode:  6 Epochs:  52  Reward: -451.2 Smooth Reward: -388.6  Epsilon: 0.4333\n",
      "Test episode:  7 Epochs:  81  Reward: -696.3 Smooth Reward: -573.7  Epsilon: 0.4333\n",
      "Test episode:  8 Epochs:  85  Reward: -822.9 Smooth Reward: -759.6  Epsilon: 0.4333\n",
      "Test episode:  9 Epochs:  51  Reward: -323.0 Smooth Reward: -572.9  Epsilon: 0.4333\n",
      "Test episode:  10 Epochs:  58  Reward: -454.0 Smooth Reward: -388.5  Epsilon: 0.4333\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.1729 binary_accuracy 0.6764 val_loss  0.1717 val_binary_accuracy 0.6894\n",
      "epoch 2\t loss  0.1658 binary_accuracy 0.6982 val_loss  0.1652 val_binary_accuracy 0.7093\n",
      "Episode:  193 Epochs:  1024  Reward: 246.7 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Episode:  194 Epochs:  1024  Reward: 253.4 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Episode:  195 Epochs:  1024  Reward: 244.2 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Episode:  196 Epochs:  1024  Reward: 241.2 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Episode:  197 Epochs:  1024  Reward: 239.8 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Episode:  198 Epochs:  1024  Reward: 239.1 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Episode:  199 Epochs:  1024  Reward: 237.8 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Episode:  200 Epochs:  1024  Reward: 242.7 Smooth Reward: 243.1  Epsilon: 0.4333\n",
      "Actor loss 0.09293022 24\n",
      "Critic loss 0.013890469 24\n",
      "Episode:  201 Epochs:  1024  Reward: 276.7 Smooth Reward: 257.6  Epsilon: 0.4203\n",
      "Episode:  202 Epochs:  1024  Reward: 270.5 Smooth Reward: 257.6  Epsilon: 0.4203\n",
      "Episode:  203 Epochs:  1024  Reward: 268.9 Smooth Reward: 257.6  Epsilon: 0.4203\n",
      "Episode:  204 Epochs:  1024  Reward: 261.1 Smooth Reward: 257.6  Epsilon: 0.4203\n",
      "Episode:  205 Epochs:  1024  Reward: 271.2 Smooth Reward: 257.6  Epsilon: 0.4203\n",
      "Episode:  206 Epochs:  1024  Reward: 278.6 Smooth Reward: 257.6  Epsilon: 0.4203\n",
      "Episode:  207 Epochs:  1024  Reward: 282.5 Smooth Reward: 257.6  Epsilon: 0.4203\n",
      "Episode:  208 Epochs:  1024  Reward: 266.8 Smooth Reward: 257.6  Epsilon: 0.4203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss -0.029988991 25\n",
      "Critic loss 0.036090434 25\n",
      "Episode:  209 Epochs:  1024  Reward: 266.6 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Episode:  210 Epochs:  1024  Reward: 300.6 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Episode:  211 Epochs:  1024  Reward: 288.8 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Episode:  212 Epochs:  1024  Reward: 261.2 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Episode:  213 Epochs:  1024  Reward: 280.3 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Episode:  214 Epochs:  1024  Reward: 266.1 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Episode:  215 Epochs:  1024  Reward: 264.4 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Episode:  216 Epochs:  1024  Reward: 274.1 Smooth Reward: 273.7  Epsilon: 0.4077\n",
      "Actor loss -0.004431309 26\n",
      "Critic loss 0.06750213 26\n",
      "Episode:  217 Epochs:  1024  Reward: 260.9 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Episode:  218 Epochs:  1024  Reward: 258.5 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Episode:  219 Epochs:  1024  Reward: 258.1 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Episode:  220 Epochs:  1024  Reward: 274.7 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Episode:  221 Epochs:  1024  Reward: 263.7 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Episode:  222 Epochs:  1024  Reward: 272.9 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Episode:  223 Epochs:  1024  Reward: 253.7 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Episode:  224 Epochs:  1024  Reward: 271.3 Smooth Reward: 269.7  Epsilon: 0.3954\n",
      "Actor loss -0.0634877 27\n",
      "Critic loss 0.11126365 27\n",
      "Test episode:  1 Epochs:  61  Reward: -563.7 Smooth Reward: -563.7  Epsilon: 0.3836\n",
      "Test episode:  2 Epochs:  83  Reward: -813.0 Smooth Reward: -688.3  Epsilon: 0.3836\n",
      "Test episode:  3 Epochs:  69  Reward: -583.2 Smooth Reward: -698.1  Epsilon: 0.3836\n",
      "Test episode:  4 Epochs:  82  Reward: -756.4 Smooth Reward: -669.8  Epsilon: 0.3836\n",
      "Test episode:  5 Epochs:  77  Reward: -503.3 Smooth Reward: -629.8  Epsilon: 0.3836\n",
      "Test episode:  6 Epochs:  98  Reward: -583.2 Smooth Reward: -543.2  Epsilon: 0.3836\n",
      "Test episode:  7 Epochs:  76  Reward: -621.5 Smooth Reward: -602.3  Epsilon: 0.3836\n",
      "Test episode:  8 Epochs:  116  Reward: -242.2 Smooth Reward: -431.8  Epsilon: 0.3836\n",
      "Test episode:  9 Epochs:  60  Reward: -544.5 Smooth Reward: -393.4  Epsilon: 0.3836\n",
      "Test episode:  10 Epochs:  52  Reward: -445.4 Smooth Reward: -494.9  Epsilon: 0.3836\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.1583 binary_accuracy 0.7178 val_loss  0.1502 val_binary_accuracy 0.7283\n",
      "epoch 2\t loss  0.1514 binary_accuracy 0.7356 val_loss  0.1432 val_binary_accuracy 0.7443\n",
      "Episode:  225 Epochs:  1024  Reward: 223.1 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Episode:  226 Epochs:  1024  Reward: 222.8 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Episode:  227 Epochs:  1024  Reward: 220.0 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Episode:  228 Epochs:  1024  Reward: 219.2 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Episode:  229 Epochs:  1024  Reward: 208.3 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Episode:  230 Epochs:  1024  Reward: 220.4 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Episode:  231 Epochs:  1024  Reward: 222.5 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Episode:  232 Epochs:  1024  Reward: 215.3 Smooth Reward: 219.0  Epsilon: 0.3836\n",
      "Actor loss 0.00871482 28\n",
      "Critic loss 0.06901567 28\n",
      "Episode:  233 Epochs:  1024  Reward: 213.2 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Episode:  234 Epochs:  1024  Reward: 207.3 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Episode:  235 Epochs:  1024  Reward: 209.0 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Episode:  236 Epochs:  1024  Reward: 205.7 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Episode:  237 Epochs:  1024  Reward: 196.7 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Episode:  238 Epochs:  1024  Reward: 210.5 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Episode:  239 Epochs:  1024  Reward: 222.6 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Episode:  240 Epochs:  1024  Reward: 204.8 Smooth Reward: 213.8  Epsilon: 0.3721\n",
      "Actor loss 0.105998255 29\n",
      "Critic loss 0.10767487 29\n",
      "Episode:  241 Epochs:  1024  Reward: 205.5 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Episode:  242 Epochs:  1024  Reward: 204.3 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Episode:  243 Epochs:  1024  Reward: 196.7 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Episode:  244 Epochs:  1024  Reward: 205.7 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Episode:  245 Epochs:  1024  Reward: 197.2 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Episode:  246 Epochs:  1024  Reward: 208.4 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Episode:  247 Epochs:  1024  Reward: 201.1 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Episode:  248 Epochs:  1024  Reward: 210.5 Smooth Reward: 206.2  Epsilon: 0.3609\n",
      "Actor loss 0.18007307 30\n",
      "Critic loss 0.0248672 30\n",
      "Episode:  249 Epochs:  1024  Reward: 197.0 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Episode:  250 Epochs:  1024  Reward: 205.1 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Episode:  251 Epochs:  1024  Reward: 200.6 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Episode:  252 Epochs:  1024  Reward: 194.6 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Episode:  253 Epochs:  1024  Reward: 204.0 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Episode:  254 Epochs:  1024  Reward: 191.9 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Episode:  255 Epochs:  1024  Reward: 197.6 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Episode:  256 Epochs:  1024  Reward: 200.3 Smooth Reward: 201.3  Epsilon: 0.3501\n",
      "Actor loss 0.08557857 31\n",
      "Critic loss 0.013006333 31\n",
      "Test episode:  1 Epochs:  53  Reward: -359.3 Smooth Reward: -359.3  Epsilon: 0.3396\n",
      "Test episode:  2 Epochs:  74  Reward: -744.8 Smooth Reward: -552.0  Epsilon: 0.3396\n",
      "Test episode:  3 Epochs:  67  Reward: -579.3 Smooth Reward: -662.0  Epsilon: 0.3396\n",
      "Test episode:  4 Epochs:  84  Reward: -752.5 Smooth Reward: -665.9  Epsilon: 0.3396\n",
      "Test episode:  5 Epochs:  55  Reward: -429.3 Smooth Reward: -590.9  Epsilon: 0.3396\n",
      "Test episode:  6 Epochs:  64  Reward: -488.1 Smooth Reward: -458.7  Epsilon: 0.3396\n",
      "Test episode:  7 Epochs:  68  Reward: -519.2 Smooth Reward: -503.6  Epsilon: 0.3396\n",
      "Test episode:  8 Epochs:  83  Reward: -783.0 Smooth Reward: -651.1  Epsilon: 0.3396\n",
      "Test episode:  9 Epochs:  55  Reward: -417.1 Smooth Reward: -600.1  Epsilon: 0.3396\n",
      "Test episode:  10 Epochs:  80  Reward: -740.5 Smooth Reward: -578.8  Epsilon: 0.3396\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.1435 binary_accuracy 0.7509 val_loss  0.1466 val_binary_accuracy 0.7588\n",
      "epoch 2\t loss  0.1366 binary_accuracy 0.7643 val_loss  0.1402 val_binary_accuracy 0.7711\n",
      "Episode:  257 Epochs:  1024  Reward: 172.3 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Episode:  258 Epochs:  1024  Reward: 168.9 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Episode:  259 Epochs:  1024  Reward: 165.8 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Episode:  260 Epochs:  1024  Reward: 167.5 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Episode:  261 Epochs:  1024  Reward: 187.8 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Episode:  262 Epochs:  1024  Reward: 167.4 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Episode:  263 Epochs:  1024  Reward: 174.9 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Episode:  264 Epochs:  1024  Reward: 174.8 Smooth Reward: 172.4  Epsilon: 0.3396\n",
      "Actor loss -0.009377209 32\n",
      "Critic loss 0.016054206 32\n",
      "Episode:  265 Epochs:  1024  Reward: 159.6 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Episode:  266 Epochs:  1024  Reward: 176.9 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Episode:  267 Epochs:  1024  Reward: 160.3 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Episode:  268 Epochs:  1024  Reward: 173.9 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Episode:  269 Epochs:  1024  Reward: 167.7 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Episode:  270 Epochs:  1024  Reward: 171.2 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Episode:  271 Epochs:  1024  Reward: 162.2 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Episode:  272 Epochs:  1024  Reward: 168.8 Smooth Reward: 170.0  Epsilon: 0.3294\n",
      "Actor loss 0.16570988 33\n",
      "Critic loss 0.006539791 33\n",
      "Episode:  273 Epochs:  1024  Reward: 169.5 Smooth Reward: 171.1  Epsilon: 0.3195\n",
      "Episode:  274 Epochs:  1024  Reward: 175.2 Smooth Reward: 171.1  Epsilon: 0.3195\n",
      "Episode:  275 Epochs:  1024  Reward: 177.1 Smooth Reward: 171.1  Epsilon: 0.3195\n",
      "Episode:  276 Epochs:  1024  Reward: 175.2 Smooth Reward: 171.1  Epsilon: 0.3195\n",
      "Episode:  277 Epochs:  1024  Reward: 179.8 Smooth Reward: 171.1  Epsilon: 0.3195\n",
      "Episode:  278 Epochs:  1024  Reward: 173.8 Smooth Reward: 171.1  Epsilon: 0.3195\n",
      "Episode:  279 Epochs:  1024  Reward: 162.9 Smooth Reward: 171.1  Epsilon: 0.3195\n",
      "Episode:  280 Epochs:  1024  Reward: 184.1 Smooth Reward: 171.1  Epsilon: 0.3195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss 0.2850467 34\n",
      "Critic loss 0.18974122 34\n",
      "Episode:  281 Epochs:  1024  Reward: 169.3 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Episode:  282 Epochs:  1024  Reward: 174.8 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Episode:  283 Epochs:  1024  Reward: 169.4 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Episode:  284 Epochs:  1024  Reward: 167.3 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Episode:  285 Epochs:  1024  Reward: 163.7 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Episode:  286 Epochs:  1024  Reward: 180.7 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Episode:  287 Epochs:  1024  Reward: 164.2 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Episode:  288 Epochs:  1024  Reward: 168.3 Smooth Reward: 172.2  Epsilon: 0.3099\n",
      "Actor loss 0.020746285 35\n",
      "Critic loss 0.13113049 35\n",
      "Test episode:  1 Epochs:  63  Reward: -473.7 Smooth Reward: -473.7  Epsilon: 0.3006\n",
      "Test episode:  2 Epochs:  53  Reward: -457.4 Smooth Reward: -465.5  Epsilon: 0.3006\n",
      "Test episode:  3 Epochs:  58  Reward: -534.2 Smooth Reward: -495.8  Epsilon: 0.3006\n",
      "Test episode:  4 Epochs:  60  Reward: -515.8 Smooth Reward: -525.0  Epsilon: 0.3006\n",
      "Test episode:  5 Epochs:  53  Reward: -354.9 Smooth Reward: -435.4  Epsilon: 0.3006\n",
      "Test episode:  6 Epochs:  51  Reward: -441.1 Smooth Reward: -398.0  Epsilon: 0.3006\n",
      "Test episode:  7 Epochs:  57  Reward: -519.8 Smooth Reward: -480.4  Epsilon: 0.3006\n",
      "Test episode:  8 Epochs:  65  Reward: -607.4 Smooth Reward: -563.6  Epsilon: 0.3006\n",
      "Test episode:  9 Epochs:  59  Reward: -550.8 Smooth Reward: -579.1  Epsilon: 0.3006\n",
      "Test episode:  10 Epochs:  74  Reward: -450.0 Smooth Reward: -500.4  Epsilon: 0.3006\n",
      "Training discriminator\n",
      "epoch 1\t loss  0.1296 binary_accuracy 0.7760 val_loss  0.1213 val_binary_accuracy 0.7823\n",
      "epoch 2\t loss  0.1231 binary_accuracy 0.7868 val_loss  0.1149 val_binary_accuracy 0.7924\n",
      "Episode:  289 Epochs:  1024  Reward: 156.3 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Episode:  290 Epochs:  1024  Reward: 137.9 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Episode:  291 Epochs:  1024  Reward: 146.4 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Episode:  292 Epochs:  1024  Reward: 147.8 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Episode:  293 Epochs:  1024  Reward: 160.6 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Episode:  294 Epochs:  1024  Reward: 130.7 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Episode:  295 Epochs:  1024  Reward: 143.5 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Episode:  296 Epochs:  1024  Reward: 142.1 Smooth Reward: 145.6  Epsilon: 0.3006\n",
      "Actor loss -0.07726779 36\n",
      "Critic loss 0.08180652 36\n",
      "Episode:  297 Epochs:  1024  Reward: 147.3 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Episode:  298 Epochs:  1024  Reward: 151.5 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Episode:  299 Epochs:  1024  Reward: 142.5 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Episode:  300 Epochs:  1024  Reward: 134.1 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Episode:  301 Epochs:  1024  Reward: 139.5 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Episode:  302 Epochs:  1024  Reward: 149.2 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Episode:  303 Epochs:  1024  Reward: 152.5 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Episode:  304 Epochs:  1024  Reward: 137.1 Smooth Reward: 144.9  Epsilon: 0.2916\n",
      "Actor loss -0.0401037 37\n",
      "Critic loss 0.015928363 37\n",
      "Episode:  305 Epochs:  1024  Reward: 159.3 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Episode:  306 Epochs:  1024  Reward: 141.9 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Episode:  307 Epochs:  1024  Reward: 159.4 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Episode:  308 Epochs:  1024  Reward: 149.2 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Episode:  309 Epochs:  1024  Reward: 142.4 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Episode:  310 Epochs:  1024  Reward: 136.5 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Episode:  311 Epochs:  1024  Reward: 160.7 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Episode:  312 Epochs:  1024  Reward: 139.7 Smooth Reward: 146.4  Epsilon: 0.2829\n",
      "Actor loss 0.042988956 38\n",
      "Critic loss 0.009969316 38\n",
      "Episode:  313 Epochs:  1024  Reward: 152.9 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Episode:  314 Epochs:  1024  Reward: 138.8 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Episode:  315 Epochs:  1024  Reward: 156.3 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Episode:  316 Epochs:  1024  Reward: 148.3 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Episode:  317 Epochs:  1024  Reward: 150.4 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Episode:  318 Epochs:  1024  Reward: 148.6 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Episode:  319 Epochs:  1024  Reward: 150.1 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Episode:  320 Epochs:  1024  Reward: 133.2 Smooth Reward: 148.0  Epsilon: 0.2744\n",
      "Actor loss 0.018917838 39\n",
      "Critic loss 0.005580356 39\n"
     ]
    }
   ],
   "source": [
    "print(\"Entrenamiento de agente con aprendizaje por imitación\")\n",
    "irl_problem.solve(10, render=False, max_step_epi=None, render_after=1500, skip_states=1,\n",
    "                  save_live_histogram='hist.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_problem.test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_saver.save(agent, 'agent_ppo.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
