{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272dc825",
   "metadata": {},
   "source": [
    "# Training from Images with Dueling Double Deep Q Network (DDDQN)\n",
    "\n",
    "By the end of this notebook you will know how to use images as states, preprocess the neural network input data, clip and normalize the reward function and how to define the Experience's Memory type and capacity. \n",
    "\n",
    "The environment selected for this tutorial classic Atari game: space invaders, provided by OpenAi Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import dddqn_agent\n",
    "from RL_Agent.base.utils.Memory.deque_memory import Memory as deq_m\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import gym\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "from RL_Agent.base.utils.networks import networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e424e",
   "metadata": {},
   "source": [
    "## Preprocessing and Normalization\n",
    "We want to preprocess the input images in order to reduce the dimensionality, crop the edges, convert to grayscale and normalize the pixel values. Here, we define the function to do all this stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bee1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atari_preprocess(obs):\n",
    "    # Crop and resize the image\n",
    "    obs = obs[20:200:2, ::2]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    obs = obs.mean(axis=2)\n",
    "\n",
    "    # normalize between [0, 1]\n",
    "    obs = obs / 255.\n",
    "    \n",
    "    # Pass from 2D of shape (90, 80) to 3D array of shape (90, 80, 1)\n",
    "    obs = obs[:, :, np.newaxis]\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888a093",
   "metadata": {},
   "source": [
    "We also want to clip and normalize the reward function. The next funtion normalize the reward as: reward' = log(1+reward), and clip this value between [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836270f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_norm_atari_reward(rew):\n",
    "    return np.clip(np.log(1+rew), -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2ba5c",
   "metadata": {},
   "source": [
    "## Defining the Environment\n",
    "\n",
    "We define the Gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"SpaceInvaders-v0\"\n",
    "env = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78bef0",
   "metadata": {},
   "source": [
    "Visualization of the original input and the preprocessed input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_obs = env.reset()\n",
    "aux_prep_obs = atari_preprocess(aux_obs)\n",
    "env.reset()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(aux_obs)\n",
    "plt.subplot(122)\n",
    "plt.imshow(aux_prep_obs, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca4579",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture\n",
    "\n",
    "We define the network architecture using the function \"dueling_dqn_net\" from \"RL_Agent.base.utils.networks.networks.py\" which return a dictionary. DDDQN has a particular network architecture that we have splited in three subnetworks. The first network is the common network, which recieves the input data. As you can see in the cell below, we use convolutional layers to process the image input followed by one dense layer for the common network. Here, the network is splited in two:  the advantage network and the value network. Both subnetworks recieves the output of common subnetwork and as their name said, they computes the \"advantage\" A(a,s) of take an action given an state and the \"value\" V(s) of being in a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27177a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.dueling_dqn_net(common_conv_layers=2,\n",
    "                                            common_kernel_num=[32, 32],\n",
    "                                            common_kernel_size=[3, 3],\n",
    "                                            common_kernel_strides=[2, 2],\n",
    "                                            common_conv_activation=['relu', 'relu'],\n",
    "                                            common_dense_layers=1,\n",
    "                                            common_n_neurons=[512],\n",
    "                                            common_dense_activation=['relu'],\n",
    "                                            \n",
    "                                            advantage_dense_layers=2,\n",
    "                                            advantage_n_neurons=[256, 128],\n",
    "                                            advantage_dense_activation=['relu', 'relu'],\n",
    "\n",
    "                                            value_dense_layers=2,\n",
    "                                            value_n_neurons=[256, 128],\n",
    "                                            value_dense_activation=['relu', 'relu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7cf7f",
   "metadata": {},
   "source": [
    "## Defining the RL Agent\n",
    "\n",
    "Here, we define the RL agent. In this case, we selected a DDDQN agent which is a variation over DQN.\n",
    "\n",
    "The agent is defined configuring a few parameters:\n",
    "\n",
    "* learning_rate: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network. \n",
    "* epsilon: Determines the amount of exploration (float between [0, 1]). 0 -> Full Exploitation; 1 -> Full exploration.\n",
    "* epsilon_decay: Decay factor of the epsilon. In each iteration we calculate the new epslon value as: epsilon' = epsilon * epsilon_decay.\n",
    "* esilon_min: minimun value epsilon can reach during the training procedure.\n",
    "* net_architecture: net architecture defined before.\n",
    "* n_stack: number of stacked timesteps to form the state.\n",
    "* img_input: boolean. Set to True where the states are images in form of 3D numpy arrays.\n",
    "* state_size: tuple, size of the state.\n",
    "\n",
    "Here, we have two new parameters: \n",
    "\n",
    "1) img_input is just a boolean value that need to be setted as True where the input data are images.\n",
    "\n",
    "2) state_size is the size of the input states. When is not defines the library use automaticaly the state size defined in the environmen but, as we changed it in the preprocessing, we need to set this value in an explicit way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77164ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dddqn_agent.Agent(learning_rate=1e-3,\n",
    "                          batch_size=64,\n",
    "                          epsilon=0.9,\n",
    "                          epsilon_decay=0.999999,\n",
    "                          epsilon_min=0.15,\n",
    "                          net_architecture=net_architecture,\n",
    "                          n_stack=5,\n",
    "                          img_input=True,\n",
    "                          state_size=(90, 80, 1)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdfa0c",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "Create a RL problem were the comunication between agent and environment are managed. In this case, we use the funcionality from \"RL_Problem.rl_problem.py\" which makes transparent to the user the selection of the matching problem. The function \"Problem\" automatically selects the problem based on the used agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304043d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = rl_problem.Problem(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c4f32",
   "metadata": {},
   "source": [
    "After defining the problem we are going to set the state preprocessing and reward normalization and clipping functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8cbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.preprocess = atari_preprocess\n",
    "problem.clip_norm_reward = clip_norm_atari_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24599649",
   "metadata": {},
   "source": [
    "This environment consumes too memory when storing the states (images) for training the neural network. This is a good momment to introduce how to select the memory to use and its size.\n",
    "\n",
    "DQN based methods are compatibles with all memories defined in RL_Agent.base.utils.Memory.py. Actually, you can find a deque memory, which is the standar memory for DQN methods and a Prioritized Experience Replay (PER) memory.\n",
    "\n",
    "By default DDDQN uses deque memory. In this specific case, we want to change the capacity of the experiences memory to not overflow the physic memory of the computer.\n",
    "\n",
    "All DQN based algorithms allows using both types of memory. A2C with experience memory algorithms (from RL_Agent.a2c_agent_discrete_queue.py and RL_Agent.a2c_agent_continuous_queue.py) allows using deque memory. All other algprithms use a buffer instead of a experience memory which length is set through the \"batch_size\" property of the agent class with the exception of PPO algortithms which include a \"memory_size\" property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_max_len = 1000 \n",
    "problem.agent.set_memory(deq_m, memory_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e26c7",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem that we have define. Here, we specify the number of episodes, the render boolean, the verbosity of the function, the skip_states parameter and additionaly if we want to render the environment after n iterations. \n",
    "\n",
    "When render is set to False, we can specify the \"render_after\" parameter. The environement will be rendering once the specified number of iterations was reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2700bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(episodes=5, skip_states=3, render=False, render_after=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ef3e1",
   "metadata": {},
   "source": [
    "The next cell run n iterations in fully exploitative mode to check the performance obtained by the agent. It will be rendered by default. The performance of the agent will be very bad, to reach aceptables performance we will need to run thousands of iterations for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81acdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.test(n_iter=2, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c55cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c305764",
   "metadata": {},
   "source": [
    "Run this last cell if you want to save the agent to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_saver.save(agent, 'agent_dddqn.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed27ee",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "- We learned how to use image data withing the RL agents over Atari games.\n",
    "- We learned how to preproces the states data and how to clip and normalize the reward function.\n",
    "- We learned how to change the leght and type of the exoeriences memory.\n",
    "- We learned how to select the experiences memory used by Deep Q Network based methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90048242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
