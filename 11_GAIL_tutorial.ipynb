{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a76c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serch/anaconda3/envs/capoir/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.7.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 10:22:44.260468: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2021-12-10 10:22:44.286218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2799925000 Hz\n",
      "2021-12-10 10:22:44.286658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557eaa8d4ed0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-12-10 10:22:44.286680: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-12-10 10:22:44.286788: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.utils import play\n",
    "from IL_Problem.base.utils.callbacks import Callbacks, load_expert_memories\n",
    "from RL_Agent import dddqn_agent, ppo_agent_discrete_parallel\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from IL_Problem.base.utils.networks import networks_dictionaries as il_networks\n",
    "from RL_Problem import rl_problem as rl_p\n",
    "from IL_Problem.deepirl import DeepIRL\n",
    "from IL_Problem.gail import GAIL\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb165437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version +978d2ce)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env_name =\"SpaceInvaders-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5563553",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = Callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878d8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "play.play(env, zoom=3, callback=cb.remember_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b135f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ready\n",
      "data saved\n"
     ]
    }
   ],
   "source": [
    "exp_path = \"expert_demonstrations/SpaceInvaders_expert.pkl\"\n",
    "cb.save_memories(exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431ce6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar las imágenes\n",
    "def atari_preprocess(obs):\n",
    "    # Crop and resize the image\n",
    "    obs = obs[20:200:2, ::2]\n",
    "\n",
    "    # Convert the image to greyscale\n",
    "    obs = obs.mean(axis=2)\n",
    "\n",
    "    # normalize between from 0 to 1\n",
    "    obs = obs / 255.\n",
    "    obs = obs[:, :, np.newaxis]\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff0f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos las dimensiones del estado una vez preprocesado, es necesario que el tercer eje marque el número de canales\n",
    "state_size = (90, 80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4eed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.ppo_net(actor_conv_layers=2,\n",
    "                                    actor_kernel_num=[8, 8],\n",
    "                                    actor_kernel_size=[3, 3],\n",
    "                                    actor_kernel_strides=[2, 2],\n",
    "                                    actor_conv_activation=['relu', 'relu'],\n",
    "                                    actor_dense_layers=2,\n",
    "                                    actor_n_neurons=[128, 128],\n",
    "                                    actor_dense_activation=['relu', 'relu'],\n",
    "\n",
    "                                    critic_conv_layers=2,\n",
    "                                    critic_kernel_num=[8, 8],\n",
    "                                    critic_kernel_size=[3, 3],\n",
    "                                    critic_kernel_strides=[2, 2],\n",
    "                                    critic_conv_activation=['relu', 'relu'],\n",
    "                                    critic_dense_layers=2,\n",
    "                                    critic_n_neurons=[128, 128],\n",
    "                                    critic_dense_activation=['relu', 'relu'],\n",
    "                                    use_custom_network=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f078fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent_discrete_parallel.Agent(actor_lr=1e-4,\n",
    "                                              critic_lr=1e-4,\n",
    "                                              batch_size=128,\n",
    "                                              epsilon=0.9,\n",
    "                                              epsilon_decay=0.97,\n",
    "                                              epsilon_min=0.15,\n",
    "                                              memory_size=128,\n",
    "                                              net_architecture=net_architecture,\n",
    "                                              n_stack=5,\n",
    "                                              img_input=True,\n",
    "                                              state_size=(90, 80, 1)\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60600ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8:\n",
      "Process Process-7:\n",
      "Process Process-3:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-6:\n",
      "Process Process-5:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/multiprocessing_env.py\", line 12, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/serch/anaconda3/envs/capoir/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "rl_problem = rl_p.Problem(env, agent)\n",
    "rl_problem.preprocess = atari_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbfb6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_expert_actions = True\n",
    "discriminator_stack = 5\n",
    "exp_memory = load_expert_memories(exp_path, load_action=use_expert_actions, n_stack=discriminator_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a434fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_custom_model(input_shape):\n",
    "    x_input = Input(shape=input_shape, name='disc_common_input')\n",
    "    x = Dense(128, activation='relu')(x_input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=x_input, outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f15020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_net_architecture = il_networks.irl_discriminator_net(use_custom_network=True,\n",
    "                                                         state_custom_network=None,\n",
    "                                                         common_custom_network=one_layer_custom_model,\n",
    "                                                         define_custom_output_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bec5900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serch/TFM/CAPOIRL-TF2/IL_Problem/base/il_problem_super.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.expert_traj = np.array([[np.array([self.preprocess(o) for o in x[0]]), x[1]] for x in expert_traj])\n"
     ]
    }
   ],
   "source": [
    "irl_problem = GAIL(rl_problem, exp_memory, lr_disc=1e-5, batch_size_disc=128, epochs_disc=2, val_split_disc=0.1,\n",
    "                   n_stack_disc=discriminator_stack, net_architecture=irl_net_architecture,\n",
    "                   use_expert_actions=use_expert_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e771fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1 Epochs:  128  Reward: 80.9 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Episode:  2 Epochs:  128  Reward: 81.0 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Episode:  3 Epochs:  128  Reward: 80.9 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Episode:  4 Epochs:  128  Reward: 80.9 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Episode:  5 Epochs:  128  Reward: 81.0 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Episode:  6 Epochs:  128  Reward: 81.0 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Episode:  7 Epochs:  128  Reward: 81.0 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Episode:  8 Epochs:  128  Reward: 81.0 Smooth Reward: 81.0  Epsilon: 0.9000\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.4201 mean_squared_error 0.2629 val_loss  1.3830 val_mean_squared_error 0.2583\n",
      "epoch 2\t loss  1.3634 mean_squared_error 0.2545 val_loss  1.3388 val_mean_squared_error 0.2516\n",
      "Actor loss 0.6343714 0\n",
      "Critic loss 1.247703 0\n",
      "Episode:  9 Epochs:  128  Reward: 75.1 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Episode:  10 Epochs:  128  Reward: 75.1 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Episode:  11 Epochs:  128  Reward: 75.0 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Episode:  12 Epochs:  128  Reward: 75.0 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Episode:  13 Epochs:  128  Reward: 75.1 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Episode:  14 Epochs:  128  Reward: 75.0 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Episode:  15 Epochs:  128  Reward: 75.0 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Episode:  16 Epochs:  128  Reward: 75.1 Smooth Reward: 78.0  Epsilon: 0.8730\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.3198 mean_squared_error 0.2484 val_loss  1.3021 val_mean_squared_error 0.2460\n",
      "epoch 2\t loss  1.2814 mean_squared_error 0.2432 val_loss  1.2684 val_mean_squared_error 0.2410\n",
      "Actor loss 0.16712172 1\n",
      "Critic loss 0.2930328 1\n",
      "Episode:  17 Epochs:  128  Reward: 71.8 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Episode:  18 Epochs:  128  Reward: 71.9 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Episode:  19 Epochs:  128  Reward: 71.5 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Episode:  20 Epochs:  128  Reward: 71.7 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Episode:  21 Epochs:  128  Reward: 71.8 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Episode:  22 Epochs:  128  Reward: 71.7 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Episode:  23 Epochs:  128  Reward: 71.5 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Episode:  24 Epochs:  128  Reward: 71.8 Smooth Reward: 73.4  Epsilon: 0.8468\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.2470 mean_squared_error 0.2383 val_loss  1.2454 val_mean_squared_error 0.2363\n",
      "epoch 2\t loss  1.2200 mean_squared_error 0.2339 val_loss  1.2224 val_mean_squared_error 0.2321\n",
      "Actor loss 0.12983067 2\n",
      "Critic loss 0.12451683 2\n",
      "Episode:  25 Epochs:  128  Reward: 69.6 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Episode:  26 Epochs:  128  Reward: 69.4 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Episode:  27 Epochs:  128  Reward: 69.5 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Episode:  28 Epochs:  128  Reward: 69.7 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Episode:  29 Epochs:  128  Reward: 69.7 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Episode:  30 Epochs:  128  Reward: 69.4 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Episode:  31 Epochs:  128  Reward: 69.5 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Episode:  32 Epochs:  128  Reward: 69.4 Smooth Reward: 70.6  Epsilon: 0.8214\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.1961 mean_squared_error 0.2300 val_loss  1.2005 val_mean_squared_error 0.2283\n",
      "epoch 2\t loss  1.1749 mean_squared_error 0.2264 val_loss  1.1834 val_mean_squared_error 0.2249\n",
      "Actor loss 0.15577634 3\n",
      "Critic loss 0.032242335 3\n",
      "Episode:  33 Epochs:  128  Reward: 67.9 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Episode:  34 Epochs:  128  Reward: 68.1 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Episode:  35 Epochs:  128  Reward: 68.1 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Episode:  36 Epochs:  128  Reward: 68.0 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Episode:  37 Epochs:  128  Reward: 68.2 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Episode:  38 Epochs:  128  Reward: 68.0 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Episode:  39 Epochs:  128  Reward: 68.0 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Episode:  40 Epochs:  128  Reward: 68.0 Smooth Reward: 68.8  Epsilon: 0.7968\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.1596 mean_squared_error 0.2232 val_loss  1.1488 val_mean_squared_error 0.2218\n",
      "epoch 2\t loss  1.1456 mean_squared_error 0.2202 val_loss  1.1352 val_mean_squared_error 0.2190\n",
      "Actor loss 0.1181659 4\n",
      "Critic loss 0.02648155 4\n",
      "Episode:  41 Epochs:  128  Reward: 66.8 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Episode:  42 Epochs:  128  Reward: 66.9 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Episode:  43 Epochs:  128  Reward: 66.9 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Episode:  44 Epochs:  128  Reward: 66.9 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Episode:  45 Epochs:  128  Reward: 66.9 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Episode:  46 Epochs:  128  Reward: 67.1 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Episode:  47 Epochs:  128  Reward: 66.7 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Episode:  48 Epochs:  128  Reward: 66.6 Smooth Reward: 67.4  Epsilon: 0.7729\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.1298 mean_squared_error 0.2174 val_loss  1.1726 val_mean_squared_error 0.2164\n",
      "epoch 2\t loss  1.1143 mean_squared_error 0.2150 val_loss  1.1639 val_mean_squared_error 0.2140\n",
      "Actor loss 0.08423567 5\n",
      "Critic loss 0.02267592 5\n",
      "Episode:  49 Epochs:  128  Reward: 66.1 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Episode:  50 Epochs:  128  Reward: 66.1 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Episode:  51 Epochs:  128  Reward: 66.2 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Episode:  52 Epochs:  128  Reward: 66.4 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Episode:  53 Epochs:  128  Reward: 66.2 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Episode:  54 Epochs:  128  Reward: 66.0 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Episode:  55 Epochs:  128  Reward: 66.3 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Episode:  56 Epochs:  128  Reward: 66.1 Smooth Reward: 66.5  Epsilon: 0.7497\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.1177 mean_squared_error 0.2129 val_loss  1.1210 val_mean_squared_error 0.2119\n",
      "epoch 2\t loss  1.0988 mean_squared_error 0.2109 val_loss  1.1136 val_mean_squared_error 0.2100\n",
      "Actor loss -0.03318557 6\n",
      "Critic loss 0.022003576 6\n",
      "Episode:  57 Epochs:  128  Reward: 65.6 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Episode:  58 Epochs:  128  Reward: 65.5 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Episode:  59 Epochs:  128  Reward: 65.5 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Episode:  60 Epochs:  128  Reward: 65.7 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Episode:  61 Epochs:  128  Reward: 65.7 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Episode:  62 Epochs:  128  Reward: 65.4 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Episode:  63 Epochs:  128  Reward: 65.4 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Episode:  64 Epochs:  128  Reward: 65.3 Smooth Reward: 65.8  Epsilon: 0.7272\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.0983 mean_squared_error 0.2090 val_loss  1.1217 val_mean_squared_error 0.2082\n",
      "epoch 2\t loss  1.0885 mean_squared_error 0.2072 val_loss  1.1162 val_mean_squared_error 0.2065\n",
      "Actor loss -0.023770269 7\n",
      "Critic loss 0.02153695 7\n",
      "Episode:  65 Epochs:  128  Reward: 65.4 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Episode:  66 Epochs:  128  Reward: 65.2 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Episode:  67 Epochs:  128  Reward: 65.2 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Episode:  68 Epochs:  128  Reward: 65.1 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Episode:  69 Epochs:  128  Reward: 65.4 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Episode:  70 Epochs:  128  Reward: 65.3 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Episode:  71 Epochs:  128  Reward: 65.0 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Episode:  72 Epochs:  128  Reward: 65.3 Smooth Reward: 65.4  Epsilon: 0.7054\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.0847 mean_squared_error 0.2057 val_loss  1.0881 val_mean_squared_error 0.2050\n",
      "epoch 2\t loss  1.0764 mean_squared_error 0.2042 val_loss  1.0833 val_mean_squared_error 0.2035\n",
      "Actor loss 0.14362687 8\n",
      "Critic loss 0.020155538 8\n",
      "Episode:  73 Epochs:  128  Reward: 65.1 Smooth Reward: 65.1  Epsilon: 0.6842\n",
      "Episode:  74 Epochs:  128  Reward: 64.8 Smooth Reward: 65.1  Epsilon: 0.6842\n",
      "Episode:  75 Epochs:  128  Reward: 64.9 Smooth Reward: 65.1  Epsilon: 0.6842\n",
      "Episode:  76 Epochs:  128  Reward: 64.9 Smooth Reward: 65.1  Epsilon: 0.6842\n",
      "Episode:  77 Epochs:  128  Reward: 64.9 Smooth Reward: 65.1  Epsilon: 0.6842\n",
      "Episode:  78 Epochs:  128  Reward: 65.1 Smooth Reward: 65.1  Epsilon: 0.6842\n",
      "Episode:  79 Epochs:  128  Reward: 64.8 Smooth Reward: 65.1  Epsilon: 0.6842\n",
      "Episode:  80 Epochs:  128  Reward: 64.8 Smooth Reward: 65.1  Epsilon: 0.6842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training discriminator\n",
      "epoch 1\t loss  1.0775 mean_squared_error 0.2028 val_loss  1.0785 val_mean_squared_error 0.2022\n",
      "epoch 2\t loss  1.0779 mean_squared_error 0.2015 val_loss  1.0748 val_mean_squared_error 0.2010\n",
      "Actor loss -0.071480766 9\n",
      "Critic loss 0.016517777 9\n",
      "Episode:  81 Epochs:  128  Reward: 64.6 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Episode:  82 Epochs:  128  Reward: 64.6 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Episode:  83 Epochs:  128  Reward: 64.9 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Episode:  84 Epochs:  128  Reward: 65.0 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Episode:  85 Epochs:  128  Reward: 64.9 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Episode:  86 Epochs:  128  Reward: 64.8 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Episode:  87 Epochs:  128  Reward: 65.1 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Episode:  88 Epochs:  128  Reward: 64.6 Smooth Reward: 64.9  Epsilon: 0.6637\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.0684 mean_squared_error 0.2004 val_loss  1.0660 val_mean_squared_error 0.1998\n",
      "epoch 2\t loss  1.0696 mean_squared_error 0.1992 val_loss  1.0646 val_mean_squared_error 0.1988\n",
      "Actor loss 0.16764961 10\n",
      "Critic loss 0.031474374 10\n",
      "Episode:  89 Epochs:  128  Reward: 64.6 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Episode:  90 Epochs:  128  Reward: 64.4 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Episode:  91 Epochs:  128  Reward: 64.4 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Episode:  92 Epochs:  128  Reward: 64.4 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Episode:  93 Epochs:  128  Reward: 65.0 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Episode:  94 Epochs:  128  Reward: 65.1 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Episode:  95 Epochs:  128  Reward: 64.4 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Episode:  96 Epochs:  128  Reward: 64.8 Smooth Reward: 64.7  Epsilon: 0.6438\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.0717 mean_squared_error 0.1983 val_loss  1.0443 val_mean_squared_error 0.1978\n",
      "epoch 2\t loss  1.0724 mean_squared_error 0.1973 val_loss  1.0427 val_mean_squared_error 0.1969\n",
      "Actor loss -0.14355029 11\n",
      "Critic loss 0.010568177 11\n",
      "Episode:  97 Epochs:  128  Reward: 64.5 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Episode:  98 Epochs:  128  Reward: 64.5 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Episode:  99 Epochs:  128  Reward: 64.6 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Episode:  100 Epochs:  128  Reward: 64.3 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Episode:  101 Epochs:  128  Reward: 64.5 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Episode:  102 Epochs:  128  Reward: 64.4 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Episode:  103 Epochs:  128  Reward: 64.5 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Episode:  104 Epochs:  128  Reward: 64.3 Smooth Reward: 64.5  Epsilon: 0.6245\n",
      "Training discriminator\n",
      "epoch 1\t loss  1.0616 mean_squared_error 0.1964 val_loss  1.0662 val_mean_squared_error 0.1961\n",
      "epoch 2\t loss  1.0570 mean_squared_error 0.1956 val_loss  1.0648 val_mean_squared_error 0.1952\n",
      "Actor loss -0.1831076 12\n",
      "Critic loss 0.030347943 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8857/4048056774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mirl_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step_epi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_after\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/IL_Problem/gail.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, iterations, render, render_after, max_step_epi, skip_states, verbose, save_live_histogram)\u001b[0m\n\u001b[1;32m     90\u001b[0m         self.rl_problem.solve(iterations, render=render, max_step_epi=None, render_after=None, skip_states=0,\n\u001b[1;32m     91\u001b[0m                               \u001b[0mdiscriminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_traj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert_traj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                               save_live_histogram=save_live_histogram)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/ppo_problem_base.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, episodes, render, render_after, max_step_epi, skip_states, verbose, discriminator, expert_traj, save_live_histogram, smooth_rewards)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self.collect_batch(render, render_after, max_step_epi, skip_states, verbose, discriminator, expert_traj,\n\u001b[0;32m--> 115\u001b[0;31m                                save_live_histogram)\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/ppo_problem_parallel_base.py\u001b[0m in \u001b[0;36mcollect_batch\u001b[0;34m(self, render, render_after, max_step_epi, skip_states, verbose, discriminator, expert_traj, save_live_histories)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Select an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Problem/base/rl_problem_base.py\u001b[0m in \u001b[0;36mact_train\u001b[0;34m(self, obs, obs_queue)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \"\"\"\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_stack\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_stack\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Agent/ppo_agent_discrete_parallel.py\u001b[0m in \u001b[0;36mact_train\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m    110\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_obs_act_multithread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mact_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# if np.random.rand() <= self.epsilon:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Agent/base/utils/networks/agent_networks.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "irl_problem.solve(200, render=False, max_step_epi=100, render_after=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfc63c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serch/anaconda3/envs/capoir/lib/python3.7/site-packages/gym/envs/atari/environment.py:257: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  \"We strongly suggest supplying `render_mode` when \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode:  1 Epochs:  496  Reward: 0.0 Smooth Reward: 0.0  Epsilon: 0.6057\n",
      "Test episode:  2 Epochs:  508  Reward: 0.0 Smooth Reward: 0.0  Epsilon: 0.6057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8857/1841806111.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrl_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Problem/base/PPO/ppo_problem_parallel_base.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, n_iter, render, callback, verbose, smooth_rewards)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0maux_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmooth_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Problem/base/rl_problem_base.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, n_iter, render, verbose, callback, smooth_rewards)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;31m# Select action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0mprev_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Problem/base/rl_problem_base.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, obs_queue)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \"\"\"\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_stack\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_stack\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Agent/ppo_agent_discrete_parallel.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_obs_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mact_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# action = np.argmax(p[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TFM/CAPOIRL-TF2/RL_Agent/base/utils/networks/agent_networks.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/capoir/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rl_problem.test(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea4808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
