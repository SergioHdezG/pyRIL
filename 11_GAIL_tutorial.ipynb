{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55200d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4a339",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning (GAIL)\n",
    "\n",
    "By the end of this tutorial you will know how to use the GAIL algorithm provided in this library to solve an imitation learning problem where the expert are you.\n",
    "\n",
    "If you did the 10_IRL_tutorial you almost know how to use GAIL because is very similar to DeepIRL usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a76c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import play\n",
    "from IL_Problem.base.utils.callbacks import Callbacks, load_expert_memories\n",
    "from RL_Agent import dddqn_agent, ppo_agent_discrete_parallel\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from IL_Problem.base.utils.networks import networks_dictionaries as il_networks\n",
    "from RL_Problem import rl_problem as rl_p\n",
    "from IL_Problem.deepirl import DeepIRL\n",
    "from IL_Problem.gail import GAIL\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d986f",
   "metadata": {},
   "source": [
    "# Collecting Expert Trajectories\n",
    "\n",
    "In the next cell, you have take the role of an expert and play to Space Invader to record some experiences. For recording the experiences we use the calback provided in \"IL_Problem.base.utils.callbacks.py\". Finally we use the utility from Gym to play an envirnment \"gym.utils.play\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb165437",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"SpaceInvaders-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5563553",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = Callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc33c3",
   "metadata": {},
   "source": [
    "To control the ship use \"A\" and \"D\" to move left or rigth and \"space bar\" to shoot. When you think that you have enough experiences close the environment in the cross (x) or using \"Esc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "play.play(env, zoom=3, callback=cb.remember_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bbc11",
   "metadata": {},
   "source": [
    "Save the experience to disck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"expert_demonstrations/SpaceInvaders_expert.pkl\"\n",
    "cb.save_memories(exp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48279b",
   "metadata": {},
   "source": [
    "# Define a RL Problem\n",
    "\n",
    "## Preprocessing and Normalization\n",
    "\n",
    "We want to preprocess the input images in order to reduce the dimensionality, crop the edges, convert to grayscale and normalize the pixel values. Here, we define the function to do all this stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ce6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atari_preprocess(obs):\n",
    "    # Crop and resize the image\n",
    "    obs = obs[20:200:2, ::2]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    obs = obs.mean(axis=2)\n",
    "\n",
    "    # normalize between [0, 1]\n",
    "    obs = obs / 255.\n",
    "    \n",
    "    # Pass from 2D of shape (90, 80) to 3D array of shape (90, 80, 1)\n",
    "    obs = obs[:, :, np.newaxis]\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de956539",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture\n",
    "\n",
    "We define the network architecture using the function \"ppo_net\" from \"RL_Agent.base.utils.networks.networks.py\" which return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4eed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.ppo_net(actor_conv_layers=2,\n",
    "                                    actor_kernel_num=[8, 8],\n",
    "                                    actor_kernel_size=[3, 3],\n",
    "                                    actor_kernel_strides=[2, 2],\n",
    "                                    actor_conv_activation=['relu', 'relu'],\n",
    "                                    actor_dense_layers=2,\n",
    "                                    actor_n_neurons=[128, 128],\n",
    "                                    actor_dense_activation=['relu', 'relu'],\n",
    "\n",
    "                                    critic_conv_layers=2,\n",
    "                                    critic_kernel_num=[8, 8],\n",
    "                                    critic_kernel_size=[3, 3],\n",
    "                                    critic_kernel_strides=[2, 2],\n",
    "                                    critic_conv_activation=['relu', 'relu'],\n",
    "                                    critic_dense_layers=2,\n",
    "                                    critic_n_neurons=[128, 128],\n",
    "                                    critic_dense_activation=['relu', 'relu'],\n",
    "                                    use_custom_network=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd492d",
   "metadata": {},
   "source": [
    "## Define the RL Agent\n",
    "\n",
    "Here, we define the RL agent. A using the next parameters:\n",
    "\n",
    "* actor_lr: learning rate for training the actor neural network.\n",
    "* critic_lr: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network. \n",
    "* memory_size: Size of the buffer filled with experiences in each algorithm iteration. \n",
    "* epsilon: Determines the amount of exploration (float between [0, 1]). 0 -> Full Exploitation; 1 -> Full exploration.\n",
    "* epsilon_decay: Decay factor of the epsilon. In each iteration we calculate the new epslon value as: epsilon' = epsilon * epsilon_decay.\n",
    "* esilon_min: minimun value epsilon can reach during the training procedure.\n",
    "* net_architecture: net architecture defined before.\n",
    "* n_stack: number of stacked timesteps to form the state.\n",
    "* img_input: boolean. Set to True where the states are images in form of 3D numpy arrays.\n",
    "* state_size: tuple, size of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent_discrete_parallel.Agent(actor_lr=1e-4,\n",
    "                                              critic_lr=1e-4,\n",
    "                                              batch_size=128,\n",
    "                                              memory_size=128,\n",
    "                                              epsilon=0.9,\n",
    "                                              epsilon_decay=0.97,\n",
    "                                              epsilon_min=0.15,\n",
    "                                              net_architecture=net_architecture,\n",
    "                                              n_stack=5,\n",
    "                                              img_input=True,\n",
    "                                              state_size=(90, 80, 1)\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b4ebc",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "Create a RL problem were the comunications between agent and environment are managed. In this case, we use the funcionality from \"RL_Problem.rl_problem.py\" which makes transparent to the user the selection of the matching problem. The function \"Problem\" automaticaly selects the problem based on the agent used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60600ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_problem = rl_p.Problem(env, agent)\n",
    "rl_problem.preprocess = atari_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f95696",
   "metadata": {},
   "source": [
    "# Define the IRL Problem\n",
    "\n",
    "## Loading Expert Experiences\n",
    "\n",
    "In \"IL_Problem.base.utils.callbacks.py\" we have some utilities for storing and loading expert experiences. Especifically, we use the function \"load_expert_memories\" which recieves three parameters: 1) \"path\", string with path to data. 2) \"load_action\", boolean to load or not the actions. We can performs IRL training the discriminator in differenciate only the states reached by an expert from the states reached by an agent or to differenciante the the state-action pairs from the expert and agent. 3) \"n_stack\" defines how many temporal steps will be stacked to form the state when using the discriminator. We can used stacked states in the agent and not in the discriminator or we can use it for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_expert_actions = True\n",
    "discriminator_stack = 5\n",
    "exp_memory = load_expert_memories(exp_path, load_action=use_expert_actions, n_stack=discriminator_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff3e2e",
   "metadata": {},
   "source": [
    "\n",
    "## Defining Discriminator Neural Network\n",
    "\n",
    "The procedures for defining the neural network for the discriminator are the same that those that we have seen in all past tutorials for the RL agent network. The main difference is that utilities are found inside the \"IL_Problem.base\" folder.\n",
    "\n",
    "As we did for the RL agent, we can define the neural network architecture creating a keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a434fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_custom_model(input_shape):\n",
    "    x_input = Input(shape=input_shape, name='disc_common_input')\n",
    "    x = Dense(128, activation='relu')(x_input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=x_input, outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_net_architecture = il_networks.irl_discriminator_net(use_custom_network=True,\n",
    "                                                         state_custom_network=None,\n",
    "                                                         common_custom_network=one_layer_custom_model,\n",
    "                                                         define_custom_output_layer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec21c5",
   "metadata": {},
   "source": [
    "## Build the IRL Problem\n",
    "\n",
    "As well as a RL problem, an IRL problem have some parameter detailed bellow:\n",
    "\n",
    "* rl_problem: RL problem defined before. This is formed by an environment an a RL agent.\n",
    "* expert_traj: RL problem defined before. This is formed by an environment an a RL agent.\n",
    "* lr_disc: learning rate for training the discriminator neural network.\n",
    "* batch_size_disc: Size of the batches used for training the discriminator neural network. \n",
    "* epochs_disc: Number of epochs fr training the discriminator in each algorithm iteration.\n",
    "* val_split_disc: Validation split of the data used when training the discriminator.\n",
    "* n_stack_disc: number of stacked timesteps to for the state in the discriminator input.\n",
    "* net_architecture: net architecture defined before.\n",
    "* use_expert_actions: Flag for use or not actions for training the discriminator. If true, the discriminator will recieve as input state-action pairs. If False, the discriminator will recieve as inputs states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_problem = GAIL(rl_problem, exp_memory, lr_disc=1e-5, batch_size_disc=128, epochs_disc=2, val_split_disc=0.1,\n",
    "                   n_stack_disc=discriminator_stack, net_architecture=irl_net_architecture,\n",
    "                   use_expert_actions=use_expert_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e91af9",
   "metadata": {},
   "source": [
    "## Solving the IRL Problem\n",
    "\n",
    "As we always do in these series of tutorial, lest solve the instanciated problem, in this case an IRL Problem. The parameter for this function are:\n",
    "\n",
    "- iterations: Number of GAIL iterations. GAIL is integrated into the PPO workflow and the number of iterations is equivalent to the \"episodes\" param in the solve function from RL agents. Training through GAIL algorithm is like training a PPO agent adding an extra step for training the discriminator and estimate the reward values before training the RL agent. \n",
    "- render: If render or not the environment during the process.\n",
    "- max_step_epi: Limits the number of steps  of each episode.\n",
    "- render_after: render after n iterations.\n",
    "- skip_state: State skipping technique by Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).\n",
    "            https://doi.org/10.1038/nature14236If."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e771fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_problem.solve(200, render=False, max_step_epi=100, render_after=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc63c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_problem.test(10, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923499a4",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "- We learned how to collect your own expert trajectories in an interactive way.\n",
    "- We learned how to PPO joined with GAIL.\n",
    "- We saw the especial parameters from GAIL.\n",
    "- We train a RL agent through GAIL algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcee88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
