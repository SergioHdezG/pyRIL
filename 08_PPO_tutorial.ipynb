{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f13488a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 18:42:41.651218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-30 18:42:41.674026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.674287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-11-30 18:42:41.674409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-30 18:42:41.675443: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-30 18:42:41.676420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-30 18:42:41.676572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-30 18:42:41.677631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-30 18:42:41.678238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-30 18:42:41.680539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-30 18:42:41.680615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.680914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.681143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f79ad6",
   "metadata": {},
   "source": [
    "# Fully Custom Networks with TensorFlow and Proximal Policy Oprimization\n",
    "\n",
    "In this tutorial you will learn how to configure you own custon neural network in the most versatile way. You may need to know some TensorFlow to be able to do an extension of one of our neural models and create your own computation graph. \n",
    "\n",
    "We use for this example the Proximal Policy Optimization (PPO) agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fa25e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 18:42:41.692904: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2021-11-30 18:42:41.697018: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3199980000 Hz\n",
      "2021-11-30 18:42:41.697358: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c6cc0687e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-30 18:42:41.697372: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-11-30 18:42:41.697507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.697794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-11-30 18:42:41.697827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-30 18:42:41.697838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-30 18:42:41.697847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-30 18:42:41.697856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-30 18:42:41.697866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-30 18:42:41.697875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-30 18:42:41.697884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-30 18:42:41.697930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.698446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.698700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2021-11-30 18:42:41.698730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-30 18:42:41.751508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-30 18:42:41.751524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2021-11-30 18:42:41.751531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2021-11-30 18:42:41.751649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.751925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.752169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-30 18:42:41.752397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5333 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2021-11-30 18:42:41.753644: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c6cdee8a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-30 18:42:41.753655: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shernandez/anaconda3/envs/tf2py37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from RL_Problem import rl_problem\n",
    "from RL_Agent.legacy_agents import ppo_agent_discrete\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "from RL_Agent.base.utils.networks.agent_networks import PPONet, TrainingHistory\n",
    "from RL_Agent.base.utils.networks import networks, losses, returns_calculations\n",
    "\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9d04e",
   "metadata": {},
   "source": [
    "## Create the custom network\n",
    "\n",
    "To create your own neural network It must extent the \"RLNetInterfaz\" from RL_Agent.base.utils.networks.networks_interface. This interfaz contains the minimun and mandatory parameter and funtions that a network need to work within the library. In RL_Agent.base.utils.networks.networks_interface we also have th \"RLNetModel\" class which extend \"RLNetInterfaz\" and contains some implementation of common functionalities, so create your nerwork extending from \"RLNetModel\" will be easier than extending from the interfaz.\n",
    "\n",
    "In this tutorial we are going to extend the \"PPONet\" from \"RL_Agent.utils.network.agent_networks\" which already extend \"RLNetModel\" and cotains all the funtionalities that PPO needs. We recomend to extend from the classes implemented in \"RL_Agent.utils.network.agent_networks\" if you plan to use a default RL agent from this library and extend from \"RLNetModel\" if you pretend to make a deep modification of an agent or implementing a new one.\n",
    "\n",
    "### Modification to PPONet\n",
    "\n",
    "Here we explain the modification that we are going to make to the default PPO network.\n",
    "\n",
    "#### Tensorboar summaries\n",
    "\n",
    "We want to change the information recorded with tensorboard, so we need to reimplement our own funtions to write the summaries and assing they to the functions from the class:\n",
    "* self.loss_sumaries: Write information related to the loss caculation.\n",
    "* self.rl_loss_sumaries: Write information related to auxiliar data used in loss and metrics calculation.\n",
    "* self.rl_sumaries: Write information related to the RL process like reward over epochs or epsilon values over epochs.\n",
    "\n",
    "These three functions have their default implementation in \"RL_Agent.utils.network.tensor_board_loss_functions.py\"\n",
    "and receives as inputs:\n",
    "\n",
    "* data: List of values to write in the summary.\n",
    "* names: List of sumary names for each value contained in data.\n",
    "* step: Current step of the training process. We usually use the episodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3497242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_sumaries(loss, names, step):\n",
    "    if isinstance(loss, list):\n",
    "        with tf.name_scope('Losses'):\n",
    "            for l, n in zip(loss, names):\n",
    "                tf.summary.scalar(n, l, step=step)\n",
    "\n",
    "def custom_rl_loss_sumaries(data, names, step):\n",
    "    with tf.name_scope('RL_Values'):\n",
    "        for d, n in zip(data, names):\n",
    "            with tf.name_scope(n):\n",
    "                tf.summary.histogram('histogram', d, step=step)\n",
    "                tf.summary.scalar('mean', tf.reduce_mean(d), step=step)\n",
    "                tf.summary.scalar('std', tf.math.reduce_std(d), step=step)\n",
    "                tf.summary.scalar('max', tf.reduce_max(d), step=step)\n",
    "                tf.summary.scalar('min', tf.reduce_min(d), step=step)\n",
    "\n",
    "def custom_rl_sumaries(data, names, step):\n",
    "    with tf.name_scope('RL'):\n",
    "        for d, n in zip(data, names):\n",
    "            with tf.name_scope(n):\n",
    "                tf.summary.scalar(n, d, step=step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94729ea2",
   "metadata": {},
   "source": [
    "#### Actor-Critic Neural Network modifications\n",
    "\n",
    "As we are using an Actor-Critic network we initialy need to define two networks: 1) self.actor_net and 2) self.critic_net. But, in this example, we want to implement only just one neural network to process the input data with two output heads, one for the Actor and one for the Critic. To this end, we are going to define just a single network, but this deep modification will force us to re-implement the prediction and training methods.\n",
    "\n",
    "We will use the self.actor_net param to aour single network to avoid make modifications of some other functionalities due to a name change. \n",
    "\n",
    "#### Optimizer and Loss Function\n",
    "\n",
    "We redefined the \"compile\" method to define our prefered optimizer instead of the defaul one and we select that we want to use the ppo loss for discrete action spaces (this is the default loss but here we can specify another diferent loss).\n",
    "\n",
    "#### Train and Predict\n",
    "\n",
    "We have modified the predict methos in order to return only the actions and not the state values as the original one does. \n",
    "\n",
    "Finally, we have modified the _train_step method to use only one network and remove the calls to the original variable \"self.crtitic_net\" that we do not already need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbbadb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(PPONet):\n",
    "    def __init__(self, input_shape, tensorboard_dir=None):\n",
    "        super().__init__(actor_net=self._build_net(input_shape), \n",
    "                         critic_net=None, \n",
    "                         tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        self.loss_sumaries = custom_loss_sumaries\n",
    "        self.rl_loss_sumaries = custom_rl_loss_sumaries\n",
    "        self.rl_sumaries = custom_rl_sumaries\n",
    "\n",
    "    def _build_net(self, input_shape):\n",
    "        input_data = Input(shape=input_shape)\n",
    "        lstm = LSTM(64, activation='tanh')(input_data)\n",
    "        dense = Dense(256, activation='relu')(lstm)\n",
    "        \n",
    "        # Actor head\n",
    "        act_dense = Dense(128, activation='relu')(dense)\n",
    "        act_output = Dense(4, activation=\"softmax\")(act_dense)\n",
    "        \n",
    "        # Critic Head\n",
    "        critic_dense = Dense(64, activation='relu')(dense)\n",
    "        critic_output = Dense(1, activation=\"linear\")(critic_dense)\n",
    "\n",
    "        return tf.keras.models.Model(inputs=input_data, outputs=[act_output, critic_output])\n",
    "\n",
    "\n",
    "    def compile(self, loss, optimizer, metrics=None):\n",
    "        self.loss_func_actor = losses.ppo_loss_discrete\n",
    "        self.loss_func_critic = None\n",
    "        self.optimizer_actor = tf.keras.optimizers.SGD(1e-3, momentum=0.2)\n",
    "        self.optimizer_critic = None\n",
    "        self.calculate_advantages = returns_calculations.gae\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_ = self._predict(x)\n",
    "        return y_[0].numpy()\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def _train_step(self, x, old_prediction, y, returns, advantages, stddev=None, loss_clipping=0.3,\n",
    "                   critic_discount=0.5, entropy_beta=0.001):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_ = self.actor_net(x, training=True)\n",
    "            loss_actor = self.loss_func_actor(y, y_[0], advantages, old_prediction, returns, y_[1], stddev, loss_clipping,\n",
    "                                  critic_discount, entropy_beta)\n",
    "\n",
    "        variables_actor = self.actor_net.trainable_variables\n",
    "        gradients_actor,  = tape.gradient(loss_actor, variables_actor)\n",
    "        self.optimizer_actor.apply_gradients(zip(gradients_actor, variables_actor))\n",
    "\n",
    "        return [loss_actor, 0.], [gradients_actor, 0.], [variables_actor, 0.], returns, advantages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f6232",
   "metadata": {},
   "source": [
    "In the next cell, we define the network architecture dictionario in order to pass the neural model to the agent. We do this through a function that receives the input shape. Latter we create the dictionary setting \"use_tf_custom_model\" to True, which means that we are going to use a model extended ftom the \"RLNetInterfaz\". Then, we assing the function to create the model to \"tf_custom_model\".\n",
    "\n",
    "When we set the neural network model through the \"use_tf_custom_model\" and \"tf_custom_model\" params we are required to define the output layers becaouse the \"define_custom_output_layer\" param will be overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67cb6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model_tf(input_shape):\n",
    "    return CustomNet(input_shape=input_shape, tensorboard_dir='tensorboard_logs')\n",
    "\n",
    "net_architecture = networks.ppo_net(use_tf_custom_model=True,\n",
    "                                     tf_custom_model=custom_model_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb663869",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent_discrete.Agent(batch_size=256,\n",
    "                                     memory_size=500,\n",
    "                                     epsilon=1.0,\n",
    "                                     epsilon_decay=0.9,\n",
    "                                     epsilon_min=0.15,\n",
    "                                     net_architecture=net_architecture,\n",
    "                                     n_stack=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af8e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"LunarLander-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9958cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomNet' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20705/3636163882.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproblem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/CAPOIRL-TF2/RL_Problem/rl_problem.py\u001b[0m in \u001b[0;36mProblem\u001b[0;34m(environment, agent)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0magent_globals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ppo_discrete\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mRL_Problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPPO\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mppo_problem_discrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mproblem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_problem_discrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPPOProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0magent_globals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ppo_continuous\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CAPOIRL-TF2/RL_Problem/base/PPO/ppo_problem_discrete.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, environment, agent, n_stack, img_input, state_size, model_params, saving_model_params, net_architecture)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mmodel_params\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mDictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mparams\u001b[0m \u001b[0mlike\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_define_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CAPOIRL-TF2/RL_Problem/base/PPO/ppo_problem_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, environment, agent, continuous)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_builded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# self._build_agent(agent, [self.batch_size, self.epsilon, self.epsilon_min, self.epsilon_decay, self.actor_lr, self.n_step_return], self.net_architecture)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CAPOIRL-TF2/RL_Problem/base/PPO/ppo_problem_base.py\u001b[0m in \u001b[0;36m_build_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m# self._define_agent(agent=self.agent, state_size=state_size, n_actions=self.n_actions, stack=stack,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#                           img_input=self.img_input, actor_lr=self.actor_lr, critic_lr=self.critic_lr,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CAPOIRL-TF2/RL_Problem/base/PPO/ppo_problem_discrete.py\u001b[0m in \u001b[0;36m_define_agent\u001b[0;34m(self, n_actions, state_size, stack, action_bound)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_define_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# def _define_agent(self, agent, state_size, n_actions, stack, img_input, actor_lr, critic_lr, batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CAPOIRL-TF2/RL_Agent/legacy_agents/ppo_agent_discrete.py\u001b[0m in \u001b[0;36mbuild_agent\u001b[0;34m(self, state_size, n_actions, stack)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproximal_policy_optimization_loss_discrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummies_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/CAPOIRL-TF2/RL_Agent/base/PPO_base/ppo_agent_base.py\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self, net_architecture, last_activation)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefine_output_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mactor_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mactor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactor_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactor_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomNet' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "problem = rl_problem.Problem(environment, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
