{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f79ad6",
   "metadata": {},
   "source": [
    "# Fully Custom Networks with TensorFlow and Proximal Policy Oprimization (PPO)\n",
    "\n",
    "In this tutorial you will learn how to configure you own custon neural network in the most versatile way allowed. You may need to know some TensorFlow to be able to do an extension of one of our neural models and create your own computation graph. \n",
    "\n",
    "We use for this example the Proximal Policy Optimization (PPO) agent.\n",
    "\n",
    "By the end of this notebook you will know how to extend a PPO network model and make modifications to it, create your own customized tensorboard summaries, change the optimizer, set your prefered loss function, set your prefered metric, define you own and fully customizable neural network and even creating your own custom train step appliying foward pass and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import ppo_agent_discrete\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "from RL_Agent.base.utils.networks.agent_networks import PPONet, TrainingHistory\n",
    "from RL_Agent.base.utils.networks import networks, losses, returns_calculations\n",
    "\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9d04e",
   "metadata": {},
   "source": [
    "## Create a Custom Network Model with TensorFlow\n",
    "\n",
    "To create your own neural network It must extent the \"RLNetInterfaz\" from RL_Agent.base.utils.networks.networks_interface.py. This interfaz contains the minimun and mandatory parameter and funtions that a network need to work within the library. In RL_Agent.base.utils.networks.networks_interface.py we also have the \"RLNetModel\" class which extend \"RLNetInterfaz\" and contains some implementation of common functionalities, so create your nerwork extending from \"RLNetModel\" will be easier than extending from the interfaz.\n",
    "\n",
    "In this tutorial we are going to extend the \"PPONet\" from \"RL_Agent.utils.network.agent_networks.py\" which already extend \"RLNetModel\" and cotains all the funtionalities that PPO needs. We recomend to extend from the classes implemented in \"RL_Agent.utils.network.agent_networks.py\" if you plan to use a default RL agent from this library and extend from \"RLNetModel\" if you pretend to make a deep modification of an agent or implementing a new one.\n",
    "\n",
    "### Modification to PPONet\n",
    "\n",
    "Here we explain the modification that we are going to make to the default PPO network.\n",
    "\n",
    "#### Tensorboar Summaries\n",
    "\n",
    "We want to change the information recorded with tensorboard, so we need to reimplement our own funtions to write the summaries and assing they to the functions from the class:\n",
    "* self.loss_sumaries: Write information related to the loss caculation.\n",
    "* self.rl_loss_sumaries: Write information related to auxiliar data used in loss and metrics calculation.\n",
    "* self.rl_sumaries: Write information related to the RL process like reward over epochs or epsilon values over epochs.\n",
    "\n",
    "These three functions have their default implementation in \"RL_Agent.utils.network.tensor_board_loss_functions.py\"\n",
    "and receives as inputs:\n",
    "\n",
    "* data: List of values to write in the summary.\n",
    "* names: List of sumary names for each value contained in data.\n",
    "* step: Current step of the training process. We usually use the episodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_sumaries(data, names, step):\n",
    "    if isinstance(data, list):\n",
    "        with tf.name_scope('Losses'):\n",
    "            for d, n in zip(data, names):\n",
    "                tf.summary.scalar(n, d, step=step)\n",
    "\n",
    "def custom_rl_loss_sumaries(data, names, step):\n",
    "    with tf.name_scope('RL_Values'):\n",
    "        for d, n in zip(data, names):\n",
    "            with tf.name_scope(n):\n",
    "                tf.summary.histogram('histogram', d, step=step)\n",
    "                tf.summary.scalar('mean', tf.reduce_mean(d), step=step)\n",
    "                tf.summary.scalar('std', tf.math.reduce_std(d), step=step)\n",
    "                tf.summary.scalar('max', tf.reduce_max(d), step=step)\n",
    "                tf.summary.scalar('min', tf.reduce_min(d), step=step)\n",
    "\n",
    "def custom_rl_sumaries(data, names, step):\n",
    "    with tf.name_scope('RL'):\n",
    "        for d, n in zip(data, names):\n",
    "            with tf.name_scope(n):\n",
    "                tf.summary.scalar(n, d, step=step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94729ea2",
   "metadata": {},
   "source": [
    "#### Actor-Critic Neural Network modifications\n",
    "\n",
    "As we are using an Actor-Critic network we initialy need to define two networks: 1) \"self.actor_net\" and 2) \"self.critic_net\". But, in this example, we want to implement only just one neural network to process the input data with two output heads, one for the Actor and one for the Critic. To this end, we are going to define just a single network, but this deep modification will force us to re-implement the prediction and training methods.\n",
    "\n",
    "We will use the \"self.actor_net\" parameter to store our single network to avoid make modifications of some other functionalities due to a name change. \n",
    "\n",
    "#### Optimizer and Loss Function\n",
    "\n",
    "We redefined the \"compile\" method to define our prefered optimizer instead of the defaul one and we select that we want to use the ppo loss for discrete action spaces (this is the default loss for PPO but here we could specify another diferent loss).\n",
    "\n",
    "#### Train and Predict\n",
    "\n",
    "We have modified the \"predict\" function in order to return only the actions and not the state values as the original one did. We also have modified the \"_predict_values\" function because it made use of the ctitic network.\n",
    "\n",
    "Finally, we have modified the \"_train_step\" function to use only one network and remove the calls to the original variable \"self.critic_net\" that we do not already need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbadb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(PPONet):\n",
    "    def __init__(self, input_shape, tensorboard_dir=None):\n",
    "        super().__init__(actor_net=self._build_net(input_shape), \n",
    "                         critic_net=None, \n",
    "                         tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        self.loss_sumaries = custom_loss_sumaries\n",
    "        self.rl_loss_sumaries = custom_rl_loss_sumaries\n",
    "        self.rl_sumaries = custom_rl_sumaries\n",
    "        \n",
    "        # Dummy variables for surrogate the critic variables that we do not need\n",
    "        self.dummy_loss_critic = tf.Variable(0., tf.float32)\n",
    "        variables_actor = self.actor_net.trainable_variables\n",
    "        self.dummy_var_critic = [tf.Variable(tf.zeros(var.shape), tf.float32) for var in variables_actor]\n",
    "\n",
    "    def _build_net(self, input_shape):\n",
    "        input_data = Input(shape=input_shape)\n",
    "        lstm = LSTM(64, activation='tanh')(input_data)\n",
    "        dense1 = Dense(256, activation='relu')(lstm)\n",
    "        dense2 = Dense(256, activation='relu')(dense1)\n",
    "\n",
    "        # Actor head\n",
    "        act_dense = Dense(128, activation='relu')(dense2)\n",
    "        act_output = Dense(4, activation=\"softmax\")(act_dense)\n",
    "        \n",
    "        # Critic Head\n",
    "        critic_dense = Dense(64, activation='relu')(dense2)\n",
    "        critic_output = Dense(1, activation=\"linear\")(critic_dense)\n",
    "\n",
    "        return tf.keras.models.Model(inputs=input_data, outputs=[act_output, critic_output])\n",
    "\n",
    "\n",
    "    def compile(self, loss, optimizer, metrics=None):\n",
    "        # Define loss, metric and optimizer\n",
    "        self.loss_func_actor = losses.ppo_loss_discrete\n",
    "        self.loss_func_critic = None\n",
    "        self.optimizer_actor = tf.keras.optimizers.RMSprop(1e-4)\n",
    "        self.optimizer_critic = None\n",
    "        self.calculate_advantages = returns_calculations.gae\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_ = self._predict(x)\n",
    "        return y_[0].numpy()  # Take the predicted action \n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def _predict_values(self, x):\n",
    "        y_ = self.actor_net(tf.cast(x, tf.float32), training=False)\n",
    "        return y_[1]  # Take the predicted value\n",
    "    \n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def _train_step(self, x, old_prediction, y, returns, advantages, stddev=None, loss_clipping=0.3,\n",
    "                   critic_discount=0.5, entropy_beta=0.001):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_ = self.actor_net(x, training=True)\n",
    "            \n",
    "            # Pass the corresponding actions (y_[0]) and values (y_[1]) to the loss function\n",
    "            loss_actor, loss_complement_values = self.loss_func_actor(y, \n",
    "                                                                      y_[0], \n",
    "                                                                      advantages, \n",
    "                                                                      old_prediction, \n",
    "                                                                      returns, \n",
    "                                                                      y_[1], \n",
    "                                                                      stddev, \n",
    "                                                                      loss_clipping,\n",
    "                                                                      critic_discount, \n",
    "                                                                      entropy_beta)\n",
    "\n",
    "        variables_actor = self.actor_net.trainable_variables  # Get trainable variables \n",
    "        gradients_actor = tape.gradient(loss_actor, variables_actor)  # Get gradients\n",
    "        self.optimizer_actor.apply_gradients(zip(gradients_actor, variables_actor))  # Update the network\n",
    "\n",
    "        return [loss_actor, self.dummy_loss_critic], [gradients_actor, self.dummy_var_critic], [variables_actor, self.dummy_var_critic], returns, advantages, loss_complement_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f6232",
   "metadata": {},
   "source": [
    "In the next cell, we define the network architecture dictionary in order to pass the neural model to the agent. We do this through a function that receives the input shape. Latter we create the dictionary setting \"use_tf_custom_model\" to True, which means that we are going to use a model extended ftom the \"RLNetInterfaz\". Then, we assing the function to create the model to \"tf_custom_model\".\n",
    "\n",
    "When we set the neural network model through the \"use_tf_custom_model\" and \"tf_custom_model\" params, we are required to define the output layers becouse the \"define_custom_output_layer\" param will be overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model_tf(input_shape):\n",
    "    return CustomNet(input_shape=input_shape, tensorboard_dir='tensorboard_logs')\n",
    "\n",
    "net_architecture = networks.ppo_net(use_tf_custom_model=True,\n",
    "                                     tf_custom_model=custom_model_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d3f7c",
   "metadata": {},
   "source": [
    "Memory size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb663869",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent_discrete.Agent(batch_size=64,\n",
    "                                 memory_size=500,\n",
    "                                 epsilon=0.7,\n",
    "                                 epsilon_decay=0.97,\n",
    "                                 epsilon_min=0.15,\n",
    "                                 net_architecture=net_architecture,\n",
    "                                 n_stack=4,\n",
    "                                 loss_critic_discount=0.001,\n",
    "                                 loss_entropy_beta=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf3a7c",
   "metadata": {},
   "source": [
    "## Define the Environment\n",
    "\n",
    "We chose the LunarLander environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"LunarLander-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc579b9",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "The RL problem is were the comunications between agent and environment are managed. In this case, we use the funcionalities from \"RL_Problem.rl_problem.py\" which makes transparent to the user the selection of the matching problem. The function \"Problem\" automaticaly selects the problem based on the used agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9958cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = rl_problem.Problem(environment, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fcede",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem that we have define. Here, we specify the number of episodes, the skip_states parameter, we limit the maximun number of step per episode and we want to render the process after 190 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(200, render=False, max_step_epi=200, render_after=190, skip_states=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26677296",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.test(render=False, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e651e",
   "metadata": {},
   "source": [
    "## Run Tensorboard to See the Recorded Summaries\n",
    "\n",
    "Lets see the tensorboard logs. Next cell executes the command that runs the tensorboard service. To see the result, you have to open a tab in your browser on the url that the command shows, usually http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab93956",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "- We learned how to deeply modified a PPO agent.\n",
    "- We learned how to make a complete customization of an agent neural network model.\n",
    "- We learned how to define custom tensorboar summaries.\n",
    "- We learned how to change the optimizer, the loss function, metrics and advantage calculation.\n",
    "- We learned how to modify the predict methods.\n",
    "- We learned how to modify the train method and realize the foward pass and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc06ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
