{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6616c1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Advantage Actor-Critic (DPG) and Exploration Strategies\n",
    "\n",
    "In this tutorial we use for first time an Actor-Critic agent. This kind of agents extend the Policy-Based agents with a mechanism for estimation of state values V(s), resulting on a mix between Policy and Value Based agents. They are composed of two entities: 1) the Actor, which learn the policy and proposed directly the actions and 2) the Critic, which estimates the state value V(s). Then, we have two neural networks, one for the Actor and one for the Critic. In some especific situations you may want to use just one neural network with two outputs heads, this can be done implementing your neural network extending the interfaz in RL_Agent.utils.networks.networks_interfaz.py. This funtionality will be revisited in further tutorials.\n",
    "\n",
    "Aditionally we will see how to change to a different exploration strategy based on exploration rate and how to change the way to selecting actions.\n",
    "\n",
    "By the end of this tutorial you will know how implement your own exploration strategy based on modifying the exploration rate as you need, how to modify the action selection procedure and how to use Actor-Critic agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0503ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from RL_Problem.base.ActorCritic import a2c_problem\n",
    "from RL_Agent import a2c_agent_discrete, a2c_agent_discrete_queue\n",
    "import gym\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from RL_Agent.base.utils.networks import action_selection_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eeceed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining the Neural Network Architecture\n",
    "We define the network architecture using the function \"actor_critic_net_architecture\" from \"RL_Agent.base.utils.networks.networks.py\" which return a dictionary. As we are using an Actor-Critic agent, this function will requires the user to define the parameters of both neural networks, the actor net and the critic net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b288b85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_architecture = networks.actor_critic_net_architecture(\n",
    "                    actor_dense_layers=3,                                critic_dense_layers=2,\n",
    "                    actor_n_neurons=[128, 128, 128],                     critic_n_neurons=[256, 256],\n",
    "                    actor_dense_activation=['relu', 'relu', 'relu'],     critic_dense_activation=['relu', 'relu']\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f633da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Customizing the Exploration Rate\n",
    "\n",
    "Exploration rate, also known as epsilon, can be reduced by specifying the \"epsilon_decay\" paramerter. By that way the new epsion parameter will be calculated each step as: epsilon' = epsilon * epsilon_decay. If we wanted to make a different modifiction over epsilon, we well need to define an specific function to do that. \"epsilon_decay\" parameter admits a float or a fucntion. In this example we will create a function for reducing epsilon in a linear way and doing cycles. This means when epsilon reach a minimum umbral, epsilon will be reseted to a higher value. \n",
    "\n",
    "The function that we defined bellow (\"epsilon_decay\") will need to recive as parameter \"epsilon\" and \"epsilon_min\", both beaing floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e8045",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def custom_epsilon_decay(decay_rate=0.0001, init_epsilon=1.):\n",
    "    # Create a class for introducing some aditional properties\n",
    "    class epsilon_control:\n",
    "        def __init__(self, decay_rate, init_epsilon):\n",
    "            self.decay_rate = decay_rate\n",
    "            self.init_epsilon = init_epsilon\n",
    "            self.aux_epsilon = init_epsilon\n",
    "\n",
    "    eps_control = epsilon_control(decay_rate, init_epsilon)\n",
    "    \n",
    "    # Defining the function that will modify epsilon\n",
    "    def epsilon_decay(epsilon, epsilon_min):\n",
    "        epsilon = epsilon - eps_control.decay_rate\n",
    "        if epsilon < epsilon_min:\n",
    "            eps_control.aux_epsilon = eps_control.aux_epsilon - 0.1\n",
    "            epsilon = eps_control.aux_epsilon\n",
    "            if epsilon < 0.1:\n",
    "                eps_control.aux_epsilon = eps_control.init_epsilon\n",
    "                epsilon = eps_control.init_epsilon\n",
    "\n",
    "        return epsilon\n",
    "\n",
    "    return epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80760d34",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining the Agent and Modifying the Action Selection Procedure\n",
    "\n",
    "In the next cell, we define the agent as usual. Here is where we set the epsilon decay function that we defined before, we assing it to the \"epsilon_decay\" parameter.\n",
    "\n",
    "We also modify the action selection procedure. We introduce the \"train_action_selection_options\" and \"action_selection_options\" parameters. This two parameters allows the user to select how the agent select the actions during training and how the agent select the action during test, explotation or deployment. We use the functions provided on \"RL_Agent.base.utils.networks.action_selection_options.py\" \n",
    "\n",
    "The user can specify its own function for action selection following the next interface:\n",
    "\n",
    "```python\n",
    "def function(act_pred, n_actions, epsilon=0., n_env=1, exploration_noise=1.0):\n",
    "    \"\"\"\n",
    "    :param act_pred: (nd array of floats) network predictions.\n",
    "            \n",
    "    :param n_actions: (int) number of actions. In a discrete action configuration represent \n",
    "                      the number of possibles actions. In a continuous action configuration \n",
    "                      represent the number of actions to take simultaneously.\n",
    "                    \n",
    "    :param epsilon: (float in range [0., 1.]) Exploration rate. Probability of selecting an \n",
    "                    exploitative action.  \n",
    "                \n",
    "    :param n_env: (int) Number of simultaneous environment in multithread agents. Also may \n",
    "                  be seen as the number of input states; if there is one state only an \n",
    "                  action is selected, if there is three (or multiple) states three (or multiple) \n",
    "                  actions must be selected.\n",
    "                    \n",
    "    :param exploration_noise: (float in range [0., 1.]) Multiplier of exploration rate of \n",
    "                              scale of exploration.E.g.: Used for setting the stddev when \n",
    "                              sampling from a normal distribution.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1cc79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent = a2c_agent_discrete.Agent(actor_lr=1e-2,\n",
    "                                  critic_lr=1e-2,\n",
    "                                  batch_size=128,\n",
    "                                  epsilon=1.0, \n",
    "                                  epsilon_decay=custom_epsilon_decay(decay_rate=0.0001, init_epsilon=1.),\n",
    "                                  epsilon_min=0.1,\n",
    "                                  n_step_return=15,\n",
    "                                  n_stack=4,\n",
    "                                  net_architecture=net_architecture,\n",
    "                                  loss_entropy_beta=0.002,\n",
    "                                  train_action_selection_options=action_selection_options.greedy_action,\n",
    "                                  action_selection_options=action_selection_options.argmax,\n",
    "                                  tensorboard_dir='tensorboard_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b6bdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define the environment\n",
    "\n",
    "We chose the LunarLander environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781cb869",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "environment = \"LunarLander-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef4111",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "The RL problem is were the comunications between agent and environment are managed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51edde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem = a2c_problem.A2CProblem(environment, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846781a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem that we have define. Here, we specify the number of episodes and the skip_states parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb143e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem.solve(700, render=False, skip_states=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0547c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem.test(render=True, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb44fc7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc00b4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Tensorboard to See the Recorded Summaries\n",
    "\n",
    "Lets see the tensorboard logs. Next cell executes the command that runs the tensorboard service. To see the result, you have to open a tab in your browser on the url that the command shows, usually http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea64f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee66bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Takeaways\n",
    "- We trained our first Actor-Critic.\n",
    "- We learned how to customize the exploration process.\n",
    "- We learned how to set the desired mode to select actions in training mode and exploitation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9a73d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}