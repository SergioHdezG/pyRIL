{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6616c1",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic (DPG)\n",
    "In this tutorial we use for first time an Actor-Critic agent. This kind of agents extend the Policy-Based agents with a mechanism for estimation of state values V(s), resulting on a mix between Policy and Value Based agents. They are composed of two entities: 1) the Actor which learn the policy and proposed directly the actions and 2) the Critic which estimates the state value V(s). Then, we have two neural networks, one for the Actor and one for the Critic. In some especific situations you may want to use just one neural network with two outputs heads, this can be done implementing your neural network extending the interfaz in RL_Agent.utils.networks.networks_interfaz. This funtionality will be revisited in further tutorials.\n",
    "\n",
    "\n",
    "Aditionally we will see how to define the output layers of each network when we use keras, since we let the library calculate it automatically until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0503ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
