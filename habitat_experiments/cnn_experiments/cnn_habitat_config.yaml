# Experiment name: exp_1
# Lanzar con imitacion_observations_serpiente_main_beta_explore.py

base_path: "habitat_experiments/cnn_experiments/exp1/"
#base_path: "/media/archivos/home/PycharmProjects2022/pyRIL-private-KUBUS/espermatozoides_2022_06/experiments/BetaDistribution/learning_from_obs/GAIL/ActionToAngles/2022_07_11/n4/exp_1/"
# Select snake environment. Environment name (SnakeActionToAngles, SnakeActionToAnglesOrientation, SerpienteEnvActionsToModel3Parameters, SerpienteEnvActionsToModel4Parameters).

habitat_config_path: "configs/RL/objectnav_hm3d_RL.yaml"
habitat_result_path: "images"  # Concatenated with base_path.
habitat_save_video: False
habitat_oracle_stop: True
environment: "HM3DRLEnv" # "HM3DRLEnvClip"
#############################################################
#		PPO agent hyperparameters
#############################################################

# select network from habitat_experiments.utils.neuralnets.py
use_clip: False
actor_model: actor_model_cf8k3_cf8k3_concat_d256relu_d256_relu_d5_softmax
critic_model: critic_model_cf8k3_cf8k3_concat_d256relu_d256_relu_d5_softmax

# select agent params
actor_lr: 1e-4 # float or tf.keras.optimizers.schedules
critic_lr: 1e-4 # float or tf.keras.optimizers.schedules
batch_size: 64 # int. Agent's training batch size
memory_size: 2000 # int. PPO agent's experiences memory size for each thread.
epsilon: 1.0 # float in [0, 1]. Initial exploration rate
epsilon_decay: 0.97 # float. Exploration rate decay factor or scheduler from espermatozoides.training.utils.schedules.py.
epsilon_min: 0.2 # float in [0, 1]. Minimum value the exploration rate can get.
gamma: 0.97 # float in [0, 1]. Attenuate the importance of future returns in GAE estimation.
lmbda: 0.9 # float in [0, 1]. Attenuate the importance of future returns in all returns estimations.
train_epochs: 10 # int. Trainstep of the agent for each iteration of PPO.
is_habitat: True # bool. True when using a habitat env.
img_input: True # bool. Tru if network's inputs are images.
state_size: "[(480, 640, 3), (12,)]"
n_stack: 1 # int. Number of stacked states. Allows the aget to have temporal information.
n_threads: 10 # Int or None. Number of agent threads. If is None, CPU default number of thread are selected.
tensorboard_dir: "logs" # String or False. Path to store tensorboard logs.
loss_clipping: 0.15 # float. Loss clipping factor for PPO loss function. Larger values allows larger optimization steps.
loss_critic_discount: 0.000 # float. Coeficient of inportance of critic loss in actor loss calculation.
loss_entropy_beta: 0.0 # float. Coeficient of inportance of entropy loss factor in actor loss calculation.
train_action_selection_options: greedy_action # Function from RL_Agent.base.utils.networks.action_selection_options.py. How the agent are selected in exploration (training) mode.
action_selection_options: argmax # Function from RL_Agent.base.utils.networks.action_selection_options.py. How the agent are selected in exploitation (testing) mode.

preprocess: preprocess_habitat6  # Fucntion from utils.preprocess.py

##############################################################
#		Experiment hyperparameters
##############################################################

# IL training parameters
training_epochs: 20  # int. Number of complete iterations of the imitation learning algorithm.
test_epochs: 5 # Number of test iteration for checking the learned policy after training.
render_test: True


