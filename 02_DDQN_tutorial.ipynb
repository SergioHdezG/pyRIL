{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4475dca6",
   "metadata": {},
   "source": [
    "# Define a simple Neural Network with Double Deep Q Network (DDQN)\n",
    "\n",
    "By the end of this notebook you will know how to use our interface for defining the agent's neural network architecture without knowing Tensor FLow or Keras.\n",
    "\n",
    "For this purpose, we use the Double Deep Q Network (DDQN) agent to address again the Cart Pole problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5807e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import ddqn_agent\n",
    "from RL_Problem.base.ValueBased import dqn_problem\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081e9e99",
   "metadata": {},
   "source": [
    "## Defining the Environment\n",
    "\n",
    "The next cell, show how to define the CartPole environment as were saw in tutoria 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"CartPole-v1\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8274d",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture\n",
    "\n",
    "CAPOIRL have a very simple interfaz to define neural networks based in dictionaries. This is oriented to people with low experience with neural network, those which have never used the deep learning libraries or modules compatibles with CAPOIRL (Tensor Flow and Keras) and for fast prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture =  {\"dense_lay\": 2,\n",
    "                    \"n_neurons\": [128, 128],\n",
    "                    \"dense_activation\": ['relu', 'tanh']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ff59e",
   "metadata": {},
   "source": [
    "We provide some functions to define the dictionaries without the necessity or remember all the keys. This funcionality can be imported from RL_Agent.base.utils.networks.networks.py\" and it is a compilation of functions to create dictionies compatibles with each kind of RL agent.\n",
    "\n",
    "The next cell redefines \"net_architecture\" using the specific function for DDQN that returns a dictionary equivalent to the one defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.double_dqn_net(dense_layers=2,\n",
    "                                           n_neurons=[128, 128],\n",
    "                                           dense_activation=['relu', 'tanh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c75bf8",
   "metadata": {},
   "source": [
    "## Defining the RL Agent\n",
    "\n",
    "Here we define the RL agent that we are going to use. In this case, we selected a DDQN agent which is an variation of DQN.\n",
    "\n",
    "The agent is defined configuring a few parameters:\n",
    "\n",
    "* learning_rate: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network. \n",
    "* epsilon: Determines the amount of exploration (float between [0, 1]). 0 -> Full Exploitation; 1 -> Full exploration.\n",
    "* epsilon_decay: Decay factor of the epsilon. In each iteration we calculate the new epslon value as: epsilon' = epsilon * epsilon_decay.\n",
    "* esilon_min: minimun value epsilon can reach during the training procedure.\n",
    "* n_stack: number of stacked timesteps to form the state\n",
    "* net_architecture: net architecture defined before.\n",
    "\n",
    "Here, we notice two new parameters:\n",
    "\n",
    "\"net_architecture\" is used to set the network architecture that we have defined before. In this example is a dictionarie but in latter tutorials we will see how we can define more complex networks using keras or tensorflow.\n",
    "\n",
    "\"n_ stack\" is a parameter used to incorporate temporal information into the states. By default n_stack = 1, this means the state will be formed only by the current state out of the environment. Where n_stack = n, being n > 1, the state will be formed by the n last states stacked. This means, the current state out of the environment, the state in timestep -1, in timestep -2 , ..., to timestep - (n-1). If n=5 the state will be formed by the 5 last states and will have saphe (5, state_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ddqn_agent.Agent(learning_rate=1e-3,\n",
    "                         batch_size=128,\n",
    "                         epsilon=0.4,\n",
    "                         epsilon_decay=0.999,\n",
    "                         epsilon_min=0.15,\n",
    "                         n_stack=5,\n",
    "                         net_architecture=net_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2c1e9",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "Build a RL problem were the comunication between agent and environment are managed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62088456",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = dqn_problem.DQNProblem(environment, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a7180",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem that we defined. Here, we specify the number of episodes, the render boolean, the verbosity of the function and finally the \"skip_states\" parameter. \n",
    "\n",
    "The \"skip_states\" parameter have value 1 by default, this means that the agent will select an ation every timestep to be executed in the environment. When skip_states = n being n > 1, an action selected by the agent will be executed n timesteps and then the actor will select another action. This allows a faster collection of experiences during training procedure avoiding execute the neural network each timestep. \n",
    "\n",
    "This state skipping technique is introduced by Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).\n",
    "            https://doi.org/10.1038/nature14236If."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(episodes=100, render=True, skip_states=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb784d",
   "metadata": {},
   "source": [
    "The next cell run n iterations in fully explorative mode to check the performance obtained by the agent. It will be rendered by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.test(n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ced52d",
   "metadata": {},
   "source": [
    "Using \"get_histogram_metrics\" and \"plot_reward_hist\" functions the history of rewards obtained during the epochs of the training process can be visualized. Param n_moving_average select how much time steps will be used to calculate a smothed versión of the data (blue line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304db62",
   "metadata": {},
   "source": [
    "Run this last cell if you want to save the agent to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158301d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_saver.save(agent, 'agent_ddqn_pole.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00282724",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "- We learned how to use a DDQN agent.\n",
    "- We learned how to define the neural network architecture using the interface for fast prototyping and low TensorFlow level people.\n",
    "- We learned how to stack temporal information within the states.\n",
    "- We learned how to use the state skipping technique by Mnih et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213149a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
