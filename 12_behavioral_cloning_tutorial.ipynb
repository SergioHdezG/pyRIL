{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bdece",
   "metadata": {},
   "source": [
    "# Behavioral Cloning for Pretraining Agents\n",
    "\n",
    "By the end of this tutorial you will know how to train a neural network of a selected agent through behavioral cloning to get an initial point to fine tuning the agent via RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from RL_Agent import ddpg_agent, dqn_agent, dpg_agent, a2c_agent_discrete_queue, ppo_agent_discrete, \\\n",
    "    ppo_agent_discrete_parallel, dpg_agent_continuous, a2c_agent_continuous_queue, ppo_agent_continuous,\\\n",
    "    ppo_agent_continuous_parallel, a2c_agent_continuous\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from IL_Problem.base.utils.callbacks import load_expert_memories, Callbacks\n",
    "from RL_Problem import rl_problem\n",
    "from IL_Problem.bclone import BehaviorCloning\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b3f27",
   "metadata": {},
   "source": [
    "## Collecting the Expert Experiences (only if needed)\n",
    "\n",
    "We provide an expert demosntartions dataset in \"tutorials/tf_tutorials/expert_demonstrations/ExpertLunarLander.pkl\". This dataset was created runing an already trained DPG agent over the environment.\n",
    "\n",
    "Next, we provide the code we have used to generate the dataset with a DPG agent. If you already have a dataset, you do not need to run the next cell. In this code we instantiate a RL problem to train an agent and pass some callbacks to record the experiences in test function. We provide this callbacks in \"IL_Problem.base.utils.callbacks.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd69d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"LunarLander-v2\"\n",
    "environment = gym.make(environment)\n",
    "exp_path = \"tutorials/tf_tutorials/expert_demonstrations/ExpertLunarLander.pkl\"\n",
    "net_architecture = networks.dpg_net(dense_layers=2,\n",
    "                                           n_neurons=[256, 256],\n",
    "                                           dense_activation=['relu', 'relu'])\n",
    "\n",
    "expert = dpg_agent.Agent(learning_rate=5e-4,\n",
    "                         batch_size=32,\n",
    "                         net_architecture=net_architecture)\n",
    "\n",
    "expert_problem = rl_problem.Problem(environment, expert)\n",
    "\n",
    "callback = Callbacks()\n",
    "\n",
    "expert_problem.solve(1000, render=False, max_step_epi=250, render_after=980, skip_states=3)\n",
    "expert_problem.test(render=False, n_iter=400, callback=callback.remember_callback)\n",
    "\n",
    "callback.save_memories(exp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23777579",
   "metadata": {},
   "source": [
    "## Loading the Expert Experiences\n",
    "\n",
    "In \"IL_Problem.base.utils.callbacks.py\" we have some utilities for storing and loading expert experiences. Especifically, we use the function \"load_expert_memories\" which recieves three parameters: 1) \"path\", string with path to data. 2) \"load_action\", boolean to load or not the actions. We can performs IRL training the discriminator in differenciate only the states reached by an expert from the states reached by an agent or to differenciante the the state-action pairs from the expert and agent. 3) \"n_stack\" defines how many temporal steps will be stacked to form the state when using the discriminator. We can used stacked states in the agent and not in the discriminator or we can use it for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"tutorials/tf_tutorials/expert_demonstrations/ExpertLunarLander.pkl\"\n",
    "use_action = True\n",
    "n_stack = 5\n",
    "exp_memory = load_expert_memories(exp_path, load_action=use_action, n_stack=n_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a36ded",
   "metadata": {},
   "source": [
    "## Defining the Agent's Neural Network Architecture\n",
    "\n",
    "We defined only one network architecture because both actor and critic networks will have the same architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_custom_model(input_shape):\n",
    "    actor_model = Sequential()\n",
    "    actor_model.add(LSTM(16, input_shape=input_shape, activation='tanh'))\n",
    "    actor_model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
    "    actor_model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
    "    actor_model.add(Dense(128, activation='relu'))\n",
    "    return actor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.actor_critic_net_architecture(use_custom_network=True,\n",
    "                                                        actor_custom_network=lstm_custom_model,\n",
    "                                                        critic_custom_network=lstm_custom_model\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef098ca2",
   "metadata": {},
   "source": [
    "## Define the RL Agent\n",
    "\n",
    "Here, we define the RL agent. A using the next parameters:\n",
    "\n",
    "* actor_lr: learning rate for training the actor neural network.\n",
    "* critic_lr: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network. \n",
    "* epsilon: Determines the amount of exploration (float between [0, 1]). 0 -> Full Exploitation; 1 -> Full exploration.\n",
    "* epsilon_decay: Decay factor of the epsilon. In each iteration we calculate the new epslon value as: epsilon' = epsilon * epsilon_decay.\n",
    "* esilon_min: minimun value epsilon can reach during the training procedure.\n",
    "* n_step_return:\n",
    "* net_architecture: net architecture defined before.\n",
    "* n_stack: number of stacked timesteps to form the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = a2c_agent_discrete_queue.Agent(actor_lr=1e-5,\n",
    "                                       critic_lr=1e-5,\n",
    "                                       batch_size=32,\n",
    "                                       epsilon=0.3,\n",
    "                                       epsilon_decay=0.9999,\n",
    "                                       epsilon_min=0.15,\n",
    "                                       n_step_return=15,\n",
    "                                       net_architecture=net_architecture,\n",
    "                                       n_stack=n_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05de3d",
   "metadata": {},
   "source": [
    "## Build the Behavioral Cloning Algrithm\n",
    "\n",
    "Next cell denifes the behavioral cloning entity which requires the next parameters:\n",
    "- agent: RL agent defined avobe.\n",
    "- state_size: Input dimensions to the networks.\n",
    "- n_actions: number of actions. Should be the sames as the output dimension of the network.\n",
    "- n_stack: Number of timestep stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BehaviorCloning(agent, state_size=(n_stack, environment.observation_space.shape[0]), n_actions=environment.action_space.n,\n",
    "                    n_stack=n_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = BehaviorCloning(agent, state_size=(n_stack, environment.observation_space.shape[0]), n_actions=environment.action_space.shape[0],\n",
    "#                     n_stack=n_stack, action_bounds=[-1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23aaf4",
   "metadata": {},
   "source": [
    "Prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a685cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([x[0] for x in exp_memory])\n",
    "actions = np.array([x[1] for x in exp_memory])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c56919",
   "metadata": {},
   "source": [
    "## Training Through Behavioral Cloning\n",
    "\n",
    "Lets train the agent neural network. As you may notice if you have work with TensorFlow before, the parameters are very similar to the used for in \"fit\" function from keras module. This is because we are runing a supervised training.\n",
    "\n",
    "* expert_traj_s: states are the tnput data for the training process.\n",
    "* expert_traj_a: action are the labels for the training proccess.\n",
    "* epochs: Number of training epochs.\n",
    "* batch_size: Size of the batches used for training. \n",
    "* shuffle: Shuffle or not the examples on expert_traj_s and expert_traj_a.\n",
    "* optimizer: Keras optimizer to be used in training procedure.\n",
    "* loss: Loss metrics for the training procedure.\n",
    "* metrics: Metrics for the training procedure.\n",
    "* verbose: Set verbosity of the function: 0 -> no verbosity, 1 -> batch level verbosity and 2 -> epoch level. verbosity.\n",
    "* one_hot_encode_actions: If True, expert_traj_a will be transformed into one hot encoding. If False, expert_traj_a will be no altered. Useful for discrete actions.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = bc.solve(expert_traj_s=states,\n",
    "                 expert_traj_a=actions, \n",
    "                 epochs=10, \n",
    "                 batch_size=128, \n",
    "                 shuffle=True, \n",
    "                 optimizer=Adam(learning_rate=1e-4),\n",
    "                 loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=tf.keras.metrics.MeanAbsoluteError(),\n",
    "                 verbose=2,\n",
    "                 validation_split=0.15, \n",
    "                 one_hot_encode_actions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa750c80",
   "metadata": {},
   "source": [
    "## Define the environment\n",
    "We are going to use the LunarLander environment from OpenAI Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"LunarLander-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b2a16",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "Once we have pretrain the agent, we can build a RL problem and fine tune the network to reach more reactive behavior provided by the RL framework. We use the funcionality from \"RL_Problem.rl_problem.py\" which makes transparent to the user the selection of the matching problem. The function \"Problem\" automaticaly selects the problem based on the agent used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32410a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = rl_problem.Problem(environment, agent)\n",
    "problem.test(render=True, n_iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4299308",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "As a reminder, we defined before the RL agent and all the aprameters are mantained. You may notice how we defined a very low learning rate and epsilon value, this was with the objective of only fine tune and not to learn from the beguining.\n",
    "\n",
    "```python\n",
    "agent = a2c_agent_discrete_queue.Agent(actor_lr=1e-5,\n",
    "                                       critic_lr=1e-5,\n",
    "                                       batch_size=32,\n",
    "                                       epsilon=0.0,\n",
    "                                       epsilon_decay=0.9999,\n",
    "                                       epsilon_min=0.15,\n",
    "                                       n_step_return=15,\n",
    "                                       net_architecture=net_architecture,\n",
    "                                       n_stack=n_stack)\n",
    "```\n",
    "\n",
    "Next step is solving the RL problem that we have define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0de727",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(100, render=False, skip_states=1, max_step_epi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea601f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.test(render=True, n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbaf973",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "- We trained a multithread PPO agent\n",
    "- We learned how to create an environment for approaching custom problems.\n",
    "- We learned how to use the python interface for environments and its required properties and functions.\n",
    "- We used real world data to create a trading bot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
