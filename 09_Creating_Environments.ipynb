{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78ec4a",
   "metadata": {},
   "source": [
    "# Environment Implementation Turorial\n",
    "\n",
    "By the end of this notebook you will know how to implement your oun environment for solving your own problem. For this examples, we have select a real life dataset with historical stock data from Apple company (https://www.kaggle.com/tarunpaparaju/apple-aapl-historical-stock-data) and we want to train a trading agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe329c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.env_base import EnvInterface, ActionSpaceInterface\n",
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import ppo_agent_discrete_parallel\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten\n",
    "from RL_Agent.base.utils import history_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b46c0",
   "metadata": {},
   "source": [
    "## Trading Environment\n",
    "\n",
    "The environment is an entity that implements a problem of interest on a reinforcement learning compatible way, this means as a decision making problem, more specifically, as Markov Decision Problem. \n",
    "\n",
    "Our particular problem have three posible actions: 1) buy stock, 2) sell stock and 3) ilde, do nothing. The agent have to choose the best action with the objective of maximize the profit during a sequence of 100 days. Each day the agent can take an action that is executet jut before the market closure time. In order to simplify the problem, the agent only can have one unit of stock in each time step, this means that the agent has to sell the stock before buying again.\n",
    "\n",
    "The state is formed by three elements: \n",
    "\n",
    "* Price variation: the variation of the stock prices between two consecutive days expresed in x times the last price. This is calculated as \"price_variation = price(day)/price(day-1)\" where \"day\" represent the current day.\n",
    "* Gains: Potential profit if the stock is sold. If the agent do not have bought stock, \"gains = 0\". If the agent have bought stock, \"gains = price(day)/price(buying_day)\", where \"day\" represent the current day.\n",
    "* Stock Bought: This is a flag that is set to \"0\" where the agent does not have stock in possesion and is set to \"1\" when the agent have stock in possesion.\n",
    "\n",
    "The reward function return better values where the agent sell stock with good proffit. Where the agent sell stock losing money the reward will return worse values. Otherwise the reward will be neutral. Finally, if the episode terminate with stock in possesion the reward will be proportional to the money inverted to buy that stock.\n",
    "\n",
    "Especifically, we use relatives values to the buy price and sell price \"(sell price/buy price) -1\" as reward, where the agent obtain profit \"reward > 0\" and if the agent obtain losses \"reward < 0\". In the other cases \"reward = 0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bec064",
   "metadata": {},
   "source": [
    "Before creating the environment we need to unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip tutorials/data/HistoricalQuotes.zip -d tutorials/data/HistoricalQuotes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054f6a0",
   "metadata": {},
   "source": [
    "In the next cell, we define the action space extending the \"ActionSpaceInterface\" from \"environments.env_base.py\". The action space requires setting the number of actions \"self.n\". If the action space where continuous, we would be required to define the actions bounds in properties: 1) \"self.low\", low bound and 2) \"self.high\", high bound. Both bounds are float numbers and reference the overall maximun and minumum bounds for all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_space(ActionSpaceInterface):\n",
    "    def __init__(self):\n",
    "        # Number of actions\n",
    "        self.n = 3\n",
    "\n",
    "        # buy -> buy stock, sell = sell stock y idle = do nothing.\n",
    "        self.actions = {'buy': 0,\n",
    "                        'sell': 1,\n",
    "                        'idle': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43ca70",
   "metadata": {},
   "source": [
    "The next cell implements the environment itself extending \"EnvInterface\" from \"environments.env_base.py\". \n",
    "We are required to define the next functions that complains with the OpenAI Gym interface:\n",
    "\n",
    "* reset(): Reset the environment to an initial state. Receives nothing and Returns the state (ndarray).\n",
    "* step(action): Execute an action in the environment producing a transition between the current state and the next state. Receives an action (ndarray) and Returns the next state (ndarray), reward (float), done (boolean. True if a terminal state is reached) and aditional iformation (dict). \n",
    "* render(): Render the environment in a graphical way or via command line. Receives nothing and Return nothing.\n",
    "* close(): Close the rendering window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTrading(EnvInterface):\n",
    "    \"\"\"\n",
    "    Environment for stock trading using historical stock data from Apple.\n",
    "    https://www.kaggle.com/tarunpaparaju/apple-aapl-historical-stock-data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the actions and observation spaces. They are required properties.\n",
    "        self.action_space = action_space()\n",
    "        \n",
    "        # The observation space is used by the library to know the state shape. In this case, we define a dummy \n",
    "        # (array of zeroes) because we do not need the values but we have acces to the shape.\n",
    "        self.observation_space = np.zeros(3, )\n",
    "\n",
    "        # Maximum iterations per episode.\n",
    "        self.max_iter = 99\n",
    "\n",
    "        # Load data\n",
    "        dataset = pd.read_csv(\"tutorials/data/HistoricalQuotes.csv\")\n",
    "\n",
    "        # Invert data order to haver the correct cronological order.\n",
    "        dataset = dataset.iloc[::-1]\n",
    "        print(dataset.head())\n",
    "\n",
    "        # Preprocess data\n",
    "        self.data = dataset.iloc[:, 1].values\n",
    "        self.data = [float(d.split()[0][1:]) for d in self.data]\n",
    "\n",
    "        # Show dataset data\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(range(len(self.data)), self.data)\n",
    "        ax.set(xlabel='days', ylabel='price $',\n",
    "               title='Stock Trading Data')\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Auxiliar environment variables\n",
    "        self.index_day = None  # Current day.\n",
    "        self.profit = None  # Current profit\n",
    "        self.stock_buying_price = 0. # Flag to know where the agent have stock in possesion.\n",
    "\n",
    "        random.seed()\n",
    "        \n",
    "        # Buffer of days of buying and selling for rendering purposes.\n",
    "        self.render_buy_index = []\n",
    "        self.render_sell_index = []\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state.\n",
    "        :return: observation/state. numpy array of state shape\n",
    "        \"\"\"\n",
    "        gc.collect()\n",
    "        \n",
    "        # Select a random init day.\n",
    "        self.index_day = random.randint(11, len(self.data)-101)\n",
    "        self.init_index = self.index_day\n",
    "\n",
    "        price = self.data[self.index_day]\n",
    "\n",
    "        # Create the inital state. [price_variation, gains, stock bought].\n",
    "        state = np.array([1., 1., 0.])\n",
    "\n",
    "        # Initialize control variables and buffers.\n",
    "        self.last_action = self.action_space.actions['idle']\n",
    "        self.last_state = state\n",
    "        self.last_price = price\n",
    "        self.first_price = price\n",
    "        self.last_reward = 0.\n",
    "\n",
    "        # Initializa auxiliar environment variables\n",
    "        self.iter = 0\n",
    "        self.profit = 0.\n",
    "        self.index_day += 1\n",
    "        self.stock_buying_price = 0.\n",
    "\n",
    "        # Initialize rendering variables.\n",
    "        self.render_buy_index = []\n",
    "        self.render_sell_index = []\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute the action to get the next state.\n",
    "        :param action: integer in [0, 3]\n",
    "        :return: state:   numpy array of state shape.\n",
    "                 reward: float\n",
    "                 done: bool\n",
    "                 info: dict or None\n",
    "        \"\"\"\n",
    "        price = self.data[self.index_day]\n",
    "\n",
    "        price_variation = price/self.last_price\n",
    "\n",
    "        # Terminal state if the maximun number of iterations is reached.\n",
    "        done = self.iter >= self.max_iter\n",
    "\n",
    "        # Calculate reward.\n",
    "        reward = 0.\n",
    "        profit = 0.\n",
    "        if action == self.action_space.actions['buy']:\n",
    "            if self.stock_buying_price > 0:\n",
    "                action = 4  # If we already have stock we can not buy any more. Action is \"idle\" buth we assing value 4 for rendering purposes.\n",
    "            else:\n",
    "                self.stock_buying_price = self.last_price  # Store the buying price.\n",
    "\n",
    "        elif action == self.action_space.actions['sell']:\n",
    "            if self.stock_buying_price > 0:\n",
    "                profit = self.last_price - self.stock_buying_price  # Calculate profit.\n",
    "                reward = (self.last_price/self.stock_buying_price)-1.  # Calculate reward.\n",
    "\n",
    "                self.stock_buying_price = 0.\n",
    "            else:\n",
    "                action = 3  # If we do not have stock we can not sell anything. Action is \"idle\" in this case, we assing value 3 for rendering purposes.\n",
    "        \n",
    "        # If the agent did not sell the stock onece the episode has finnished, we calculate the loss with respect to the buying price.\n",
    "        if done and self.stock_buying_price > 0.:\n",
    "            reward = (self.last_price/self.stock_buying_price)-1.\n",
    "\n",
    "        self.profit += profit\n",
    "\n",
    "        gains = price/self.stock_buying_price if self.stock_buying_price > 0. else 0.\n",
    "\n",
    "\n",
    "        # Create the inital state. [price_variation, gains, stock bought].\n",
    "        state = np.array([price_variation, gains, 1. if self.stock_buying_price > 0. else 0.])\n",
    "\n",
    "\n",
    "        self.last_state = state\n",
    "        self.last_price = price\n",
    "        self.last_action = action\n",
    "        self.last_reward = reward\n",
    "        self.iter += 1\n",
    "        self.index_day += 1\n",
    "\n",
    "        return state, reward, done, None\n",
    "\n",
    "    def close(self):\n",
    "        # Close the rendering figure\n",
    "        plt.close(1)\n",
    "\n",
    "    def render(self):\n",
    "        plt.clf()\n",
    "\n",
    "        fig = plt.figure(1)\n",
    "        ax = fig.add_subplot(2, 1, 1)\n",
    "\n",
    "\n",
    "        data = self.data\n",
    "\n",
    "        # Get the current stock price.\n",
    "        valor = data[self.index_day-2]\n",
    "        ax.plot(range(100), data[self.init_index-1: self.init_index+99])\n",
    "        ax.set(xlabel='days', ylabel='price $',\n",
    "               title='Stock Trading')\n",
    "\n",
    "        if self.last_action == self.action_space.actions['buy']:\n",
    "            marker = \"^\"\n",
    "            color = 'g'\n",
    "            self.render_buy_index.append([self.iter, valor])\n",
    "        elif self.last_action == self.action_space.actions['sell']:\n",
    "            marker = \"v\"\n",
    "            color = 'r'\n",
    "            self.render_sell_index.append([self.iter, valor])\n",
    "        elif self.last_action == 3:  # Sell, but without stock the action is transformed to idle.\n",
    "            marker = \"v\"\n",
    "            color = 'y'\n",
    "        elif self.last_action == 4:  # Buy, but we already have stock so the action is tranformed to idle.\n",
    "            marker = \"^\"\n",
    "            color = 'y'\n",
    "        else:  # 'idle'\n",
    "            marker = \"D\"\n",
    "            color = 'b'\n",
    "\n",
    "        for s in self.render_sell_index:\n",
    "            ax.plot(s[0], s[1], marker=7, color='r')\n",
    "\n",
    "        for b in self.render_buy_index:\n",
    "            ax.plot(b[0], b[1], marker=6, color='g')\n",
    "\n",
    "        ax.plot(self.iter, valor, marker=marker, color=color)\n",
    "\n",
    "        text1 = \"profit: {:.1f}\".format(self.profit)\n",
    "        text2 = \"   stock buying price: {:.1f}\".format(self.stock_buying_price)\n",
    "        text3 = \"   current price: {:.1f}\".format(valor)\n",
    "        text4 = \"   reward: {:.4f}\".format(self.last_reward)\n",
    "        ax.text(0.02, 0.95, text1 + text2 + text3 + text4, horizontalalignment='left', verticalalignment='center',\n",
    "                transform=ax.transAxes)\n",
    "        ax.grid()\n",
    "\n",
    "        # We additionally show a sliding window of the last 20 days to see what the agent sees.\n",
    "        ax2 = fig.add_subplot(2, 1, 2)\n",
    "        ax2.plot(range(self.iter-19, self.iter+1), data[self.index_day-21: self.index_day - 1])\n",
    "        ax2.set(xlabel='last 20 days', ylabel='price $',\n",
    "               title='20 days window')\n",
    "        ax2.plot(self.iter, valor, marker=marker, color=color)\n",
    "        for s in self.render_sell_index:\n",
    "            if self.iter - s[0] < 20:\n",
    "                ax2.plot(s[0], s[1], marker=\"v\", color='r')\n",
    "\n",
    "        for b in self.render_buy_index:\n",
    "            if self.iter - b[0] < 20:\n",
    "                ax2.plot(b[0], b[1], marker=\"^\", color='g')\n",
    "\n",
    "        ax2.grid()\n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e52527",
   "metadata": {},
   "source": [
    "## Define the Environment\n",
    "\n",
    "Next cell build the environment class that we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTrading()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88b6ad",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture\n",
    "\n",
    "We define the network architecture using the function \"ppo_net\" from \"RL_Agent.base.utils.networks.networks.py\" which return a dictionary. As we are using an Actor-Critic agent, this function will requires the user to define the parameters of both neural networks, the actor net and the critic net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19eb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la red neuronal de la forma mas avanzada que permite la libreria con un modelo secuencial de keras\n",
    "def actor_lstm_custom_model(input_shape):\n",
    "    actor_model = Sequential()\n",
    "    actor_model.add(LSTM(64, input_shape=input_shape, activation='tanh'))\n",
    "    actor_model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
    "    actor_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    return actor_model\n",
    "\n",
    "def critic_lstm_custom_model(input_shape):\n",
    "    critic_model = Sequential()\n",
    "    critic_model.add(LSTM(64, input_shape=input_shape, activation='tanh'))\n",
    "    critic_model.add(Dense(64, input_shape=input_shape, activation='relu'))\n",
    "    critic_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    return critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48712b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.ppo_net(use_custom_network=True,\n",
    "                                    actor_custom_network=actor_lstm_custom_model,\n",
    "                                    critic_custom_network=critic_lstm_custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd17dd",
   "metadata": {},
   "source": [
    "## Defining the RL Agent\n",
    "\n",
    "\n",
    "Here, we define the RL agent using the next parameters:\n",
    "\n",
    "* actor_lr: learning rate for training the actor neural network.\n",
    "* critic_lr: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network. \n",
    "* memory_size: Size of the buffer filled with experiences in each algorithm iteration. \n",
    "* epsilon: Determines the amount of exploration (float between [0, 1]). 0 -> Full Exploitation; 1 -> Full exploration.\n",
    "* epsilon_decay: Decay factor of the epsilon. In each iteration we calculate the new epslon value as: epsilon' = epsilon * epsilon_decay.\n",
    "* esilon_min: minimun value epsilon can reach during the training procedure.\n",
    "* net_architecture: net architecture defined before.\n",
    "* n_stack: number of stacked timesteps to form the state.\n",
    "* loss_critic_discount: Discount factor for the loss comming from the critic in the actor net calculation.\n",
    "* loss_entropy_beta: Discount factor for the entropy term of the loss function.\n",
    "* tensorboard_dir: path to folder for store tensorboard summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955566cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent_discrete_parallel.Agent(actor_lr=1e-3,\n",
    "                                         critic_lr=1e-3,\n",
    "                                         batch_size=128,\n",
    "                                         memory_size=100,\n",
    "                                         epsilon=1.0,\n",
    "                                         epsilon_decay=0.97,\n",
    "                                         epsilon_min=0.15,\n",
    "                                         net_architecture=net_architecture,\n",
    "                                         n_stack=5,\n",
    "                                         loss_critic_discount=0.001,\n",
    "                                         loss_entropy_beta=0.01,\n",
    "                                         tensorboard_dir='tensorboard_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43dd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from RL_Agent import dpg_agent\n",
    "\n",
    "# net_architecture = networks.dpg_net(use_custom_network=True,\n",
    "#                                     custom_network=actor_lstm_custom_model)\n",
    "# agent = dpg_agent.Agent(learning_rate=1e-4,\n",
    "#                         batch_size=64,\n",
    "#                         net_architecture=net_architecture,\n",
    "#                         n_stack=20,\n",
    "#                        tensorboard_dir='tensorboard_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f582a",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "Create a RL problem were the comunications between agent and environment are managed. In this case, we use the funcionality from \"RL_Problem.rl_problem.py\" which makes transparent to the user the selection of the matching problem. The function \"Problem\" automaticaly selects the problem based on the agent used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = rl_problem.Problem(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52441e",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem that we have define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8930f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(1000, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc28a5",
   "metadata": {},
   "source": [
    "Lest see the performance of the trained agent. To correctly see the execution of this environment we need to run matplotlib on window mode. We do this by runing the \"%matplotlib qt\" instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ab50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "problem.test(render=False, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93af4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a4653",
   "metadata": {},
   "source": [
    "## Run Tensorboard to See the Recorded Summaries\n",
    "\n",
    "Lets see the tensorboard logs. Next cell executes the command that runs the tensorboard service. To see the result, you have to open a tab in your browser on the url that the command shows, usually http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586839ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ccfc1",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "- We trained a multithread PPO agent\n",
    "- We learned how to create an environment for approaching custom problems.\n",
    "- We learned how to use the python interface for environments and its required properties and functions.\n",
    "- We used real world data to create a trading bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911311d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
