{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe78ec4a",
   "metadata": {},
   "source": [
    "# Environment Implementation Turorial\n",
    "\n",
    "In this tutorial we show how to implement your oun environment with your own problem. For this examples we have select a real life dataset with historical stock data from Apple company (https://www.kaggle.com/tarunpaparaju/apple-aapl-historical-stock-data) and we want to train a trading agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe329c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.env_base import EnvInterface, ActionSpaceInterface\n",
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import ppo_agent_discrete_parallel\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten\n",
    "from RL_Agent.base.utils import history_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfab1b6",
   "metadata": {},
   "source": [
    "Data from https://www.kaggle.com/tarunpaparaju/apple-aapl-historical-stock-data\n",
    "\n",
    "En este tutorial vamos a crear un entorno para realizar inversiones en bolsa. Vamos a desarrollar el entorno sobre un\n",
    "conjunto de datos reales y nuestro agente tendrá que decidir cuando conviene comprar y cuando conviene vender. Para\n",
    "simplificar el problema para este ejemplo solo vamos a permitir al agente comprar una acción (que vamos a llamar stock\n",
    "a partir de ahora para no confundirlas con las acciones del agente), por lo tanto, una vez ha comprado una unidad de\n",
    "stock no podrá volver a comprar hasta que no decida vender la unidad de stock. Estamos ante un problema con acciones\n",
    "discretas por lo que podemos usar un agente DQN y las acciones disponibles para el agente son: comprar (Buy), vender\n",
    "(sell) y no hacer nada (idle), esta última puede referirse a esperar el momento idóneo para comprar o esperar el\n",
    "momento idóneo para vender. El estado esta compuesto por una tupla (variación del precio stock, variación del\n",
    "precio de stock comprado, flag stock en posesión). La variación del precio stock se refiere a la diferencia del precio\n",
    "que había en el instante anterior y el actual; la variación del precio de stock comprado se refiere a la diferencia\n",
    "entre el precio al que se adquirió stock y el precio actual, si no se dispone de stock porque todavía no se ha\n",
    "comprado nada o porque se ha vendido esta característica tomará valor cero y por último, flag stock en posesión es una\n",
    "característica binaria que si indica si tenemos stock en posesión, cuando tenemos stock vale 1 y en caso contrario 0.\n",
    "Para que el agente tenga información temporal sobre como evoluciona el precio y de las decisiones que ha ido tomando\n",
    "se apilan 20 estados para formar cada observación. Los datos se desarollan a lo largo de 10 años y para cada episodio\n",
    "seleccionamos un dia de inicio aleatorio, el episodio se prolongará durante 100 iteraciones que se corresponderán con\n",
    "los 100 días siguientes al seleccionados.\n",
    "La funcion de recompensa premiará al agente cuando gane dinero por vender stock y le penalizará cuando lo pierda.\n",
    "Por ello las acciones Buy y Idle se puentuan con reward = 0, la acción Sell se puntúa con la diferencia entre el\n",
    "precio de compra del stock y el precio por el que se ha vendido. Cuando se llega a un estado final y el agente no ha\n",
    "vendido todas sus acciones recive una penalización igual al dinero que perdería al venderlas en en el momento de\n",
    "llegar al estado final, si en ese caso tueviese ganancias la recompensa será 0.\n",
    "\n",
    "Definimos el espacio de acciones heredando de  environments.env_base.ActionSpaceInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b46c0",
   "metadata": {},
   "source": [
    "## Trading Environment\n",
    "\n",
    "The environment is an entity that implements a problem of interest on a reinforcement learning compatible way, this is as a decision making problem, more specifically, as Markov Decision Problem. \n",
    "\n",
    "Our particular problem have three posible actions: 1) buy stock, 2) sell stock and 3) ilde, do nothing The agent have to choose the best action with the objective of maximize the profit during a sequence of 100 days. Each day the agent can take an action taht is executet jut before the market closure time. To simplify the problem the agent only can have one unit of stock in each time step, this means that the agent has to sell the stock before buying again.\n",
    "\n",
    "The state if formed by three elements: \n",
    "\n",
    "* Price variation: the variation of the stock prices between two consecutive days expresed in x times the last price. This is calculated as \"price_variation = price(day)/price(day-1)\" where \"day\" represent the current day.\n",
    "* Gains: Potential profit if the stock is sold. If the agent do not have bought stock, \"gains = 0\". If the agent have bought stock, \"gains = price(day)/price(buying_day)\", where \"day\" represent the current day.\n",
    "* Stock Bought: This is a flag that is set to \"0.\" where the agent does not have stock in possesion and is set to \"1.\" when the agent have stock in possesion.\n",
    "\n",
    "The reward function return better values where the agent sell stock with good proffit. Where the agent sell stock losing money the reward will return worse values. Otherwise the reward will be neutral. Finally, if the episode terminate with stock in possesion the reward will be proportional to the money inverted to buy that stock.\n",
    "\n",
    "Especifically, we use relatives values to the buy price and sell price \"buy price/sell price\" as reward, where the agent obtain profit \"reward > 1\" and if the agent obtain losses \"reward < 1\". In the other cases \"reward = 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054f6a0",
   "metadata": {},
   "source": [
    "In the next cell we define the action space extending the \"ActionSpaceInterface\" from \"environments.env_base.py\". The action space requires tto set the number of actions \"self.n\". If the action space where continuous, we would be required to define the actions bounds in properties: 1) \"self.low\", low bound and 2) \"self.high\", high bound. Both bounds are float numbers and reference the overall maximun and minumum bounds for all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_space(ActionSpaceInterface):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        En un estorno con acciones discretas debemos definir el número de acciones self.n y un diccionario con las\n",
    "        posibles acciones self.actions.\n",
    "        \"\"\"\n",
    "        # Number of actions\n",
    "        self.n = 3\n",
    "\n",
    "        # buy -> buy stock, sell = sell stock y idle = do nothing.\n",
    "        self.actions = {'buy': 0,\n",
    "                        'sell': 1,\n",
    "                        'idle': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43ca70",
   "metadata": {},
   "source": [
    "The next cell implements the environment itself extending from \"EnvInterface\" from \"environments.env_base.py\". \n",
    "We are required to define the next functions that complains with the OpenAI Gym interface:\n",
    "\n",
    "* reset(): Reset the environment to an initial state. Receives nothing and Returns the state (ndarray).\n",
    "* step(action): Execute an action in the environment producing a transition between the current state and the next state. Receives an action (ndarray) and Returns the next state (ndarray), reward (float), done (boolean. True if a terminal state is reached) and aditional iformation (dict). \n",
    "* render(): Render the environment in a graphical way or via command line. Receives nothing and Return nothing.\n",
    "* close(): Close the rendering window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el entorno\n",
    "env = StockTrading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19eb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la red neuronal de la forma mas avanzada que permite la libreria con un modelo secuencial de keras\n",
    "def actor_lstm_custom_model(input_shape):\n",
    "    actor_model = Sequential()\n",
    "    actor_model.add(LSTM(128, input_shape=input_shape, activation='tanh'))\n",
    "    actor_model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
    "    actor_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    return actor_model\n",
    "\n",
    "def critic_lstm_custom_model(input_shape):\n",
    "    critic_model = Sequential()\n",
    "    critic_model.add(LSTM(128, input_shape=input_shape, activation='tanh'))\n",
    "    critic_model.add(Dense(64, input_shape=input_shape, activation='relu'))\n",
    "    critic_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    return critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48712b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formateamos la red para pasarsela al aagente\n",
    "net_architecture = networks.ppo_net(use_custom_network=True,\n",
    "                                    actor_custom_network=actor_lstm_custom_model,\n",
    "                                    critic_custom_network=critic_lstm_custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTrading(EnvInterface):\n",
    "    \"\"\"\n",
    "    Environment for stock trading using historical stock data from Apple.\n",
    "    https://www.kaggle.com/tarunpaparaju/apple-aapl-historical-stock-data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the actions and observation spaces. They are required properties.\n",
    "        self.action_space = action_space()\n",
    "        \n",
    "        # The observation space is used by the library to know the state shape. In this case, we define a dummy \n",
    "        # (array of zeroes) because we do not need the values but we have acces to the shape.\n",
    "        self.observation_space = np.zeros(3, )\n",
    "\n",
    "        # Maximum iterations per episode.\n",
    "        self.max_iter = 99\n",
    "\n",
    "        # Load data\n",
    "        dataset = pd.read_csv(\"/home/serch/TFM/CAPOIRL-TF2/tutorials/data/HistoricalQuotes.csv\")\n",
    "\n",
    "        # Invert data order to haver the correct cronological order.\n",
    "        dataset = dataset.iloc[::-1]\n",
    "        print(dataset.head())\n",
    "\n",
    "        # Preprocess data\n",
    "        self.data = dataset.iloc[:, 1].values\n",
    "        self.data = [float(d.split()[0][1:]) for d in self.data]\n",
    "\n",
    "        # Show dataset data\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.plot(range(len(self.data)), self.data)\n",
    "        ax.set(xlabel='days', ylabel='price $',\n",
    "               title='Stock Trading Data')\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Auxiliar environment variables\n",
    "        self.index_day = None  # Current day.\n",
    "        self.profit = None  # Current profit\n",
    "        self.stock_buying_price = 0. # Flag to know where the agent have stock in possesion.\n",
    "\n",
    "        random.seed()\n",
    "        \n",
    "        # Buffer of days of buying and selling for rendering purposes.\n",
    "        self.render_buy_index = []\n",
    "        self.render_sell_index = []\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state.\n",
    "        :return: observation/state. numpy array of state shape\n",
    "        \"\"\"\n",
    "        gc.collect()\n",
    "        \n",
    "        # Select a random init day.\n",
    "        self.index_day = random.randint(11, len(self.data)-101)\n",
    "        self.init_index = self.index_day\n",
    "\n",
    "        price = self.data[self.index_day]\n",
    "\n",
    "        # Create the inital state. [price_variation, gains, stock bought].\n",
    "        state = np.array([1., 1., 0.])\n",
    "\n",
    "        # Initialize control variables and buffers.\n",
    "        self.last_action = self.action_space.actions['idle']\n",
    "        self.last_state = state\n",
    "        self.last_price = price\n",
    "        self.first_price = price\n",
    "        self.last_reward = 0.\n",
    "\n",
    "        # Initializa auxiliar environment variables\n",
    "        self.iter = 0\n",
    "        self.profit = 0.\n",
    "        self.index_day += 1\n",
    "        self.stock_buying_price = 0.\n",
    "\n",
    "        # Initialize rendering variables.\n",
    "        self.render_buy_index = []\n",
    "        self.render_sell_index = []\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute the action to get the next state.\n",
    "        :param action: integer in [0, 3]\n",
    "        :return: state:   numpy array of state shape.\n",
    "                 reward: float\n",
    "                 done: bool\n",
    "                 info: dict or None\n",
    "        \"\"\"\n",
    "        price = self.data[self.index_day]\n",
    "\n",
    "        price_variation = price/self.last_price\n",
    "\n",
    "        # Terminal state if the maximun number of iterations is reached.\n",
    "        done = self.iter >= self.max_iter\n",
    "\n",
    "        # Calculate reward.\n",
    "        reward = 1.\n",
    "        profit = 0.\n",
    "        if action == self.action_space.actions['buy']:\n",
    "            if self.stock_buying_price > 0:\n",
    "                action = 4  # If we already have stock we can not buy any more. Action is \"idle\" buth we assing value 4 for rendering purposes.\n",
    "            else:\n",
    "                self.stock_buying_price = self.last_price  # Store the buying price.\n",
    "\n",
    "        elif action == self.action_space.actions['sell']:\n",
    "            if self.stock_buying_price > 0:\n",
    "                profit = self.last_price - self.stock_buying_price  # Calculate profit.\n",
    "                reward = self.last_price/self.stock_buying_price  # Calculate reward.\n",
    "\n",
    "                self.stock_buying_price = 0.\n",
    "            else:\n",
    "                action = 3  # If we do not have stock we can not sell anything. Action is \"idle\" in this case, we assing value 3 for rendering purposes.\n",
    "        \n",
    "        # If the agent did not sell the stock onece the episode has finnished, we calculate the loss with respect to the buying price.\n",
    "        if done and self.stock_buying_price > 0.:\n",
    "            reward = self.last_price/self.stock_buying_price\n",
    "\n",
    "        self.profit += profit\n",
    "\n",
    "        gains = price/self.stock_buying_price if self.stock_buying_price > 0. else 0.\n",
    "\n",
    "\n",
    "        # Create the inital state. [price_variation, gains, stock bought].\n",
    "        state = np.array([price_variation, gains, 1. if self.stock_buying_price > 0. else 0.])\n",
    "\n",
    "\n",
    "        self.last_state = state\n",
    "        self.last_price = price\n",
    "        self.last_action = action\n",
    "        self.last_reward = reward\n",
    "        self.iter += 1\n",
    "        self.index_day += 1\n",
    "\n",
    "        return state, reward, done, None\n",
    "\n",
    "    def close(self):\n",
    "        # Close the rendering figure\n",
    "        plt.close(1)\n",
    "\n",
    "    def render(self):\n",
    "        plt.clf()\n",
    "\n",
    "        fig = plt.figure(1)\n",
    "        ax = fig.add_subplot(2, 1, 1)\n",
    "\n",
    "\n",
    "        data = self.data\n",
    "\n",
    "        # Get the current stock price.\n",
    "        valor = data[self.index_day-2]\n",
    "        ax.plot(range(100), data[self.init_index-1: self.init_index+99])\n",
    "        ax.set(xlabel='days', ylabel='price $',\n",
    "               title='Stock Trading')\n",
    "\n",
    "        if self.last_action == self.action_space.actions['buy']:\n",
    "            marker = \"^\"\n",
    "            color = 'g'\n",
    "            self.render_buy_index.append([self.iter, valor])\n",
    "        elif self.last_action == self.action_space.actions['sell']:\n",
    "            marker = \"v\"\n",
    "            color = 'r'\n",
    "            self.render_sell_index.append([self.iter, valor])\n",
    "        elif self.last_action == 3:  # Sell, but without stock the action is transformed to idle.\n",
    "            marker = \"v\"\n",
    "            color = 'y'\n",
    "        elif self.last_action == 4:  # Buy, but we already have stock so the action is tranformed to idle.\n",
    "            marker = \"^\"\n",
    "            color = 'y'\n",
    "        else:  # 'idle'\n",
    "            marker = \"D\"\n",
    "            color = 'b'\n",
    "\n",
    "        for s in self.render_sell_index:\n",
    "            ax.plot(s[0], s[1], marker=7, color='r')\n",
    "\n",
    "        for b in self.render_buy_index:\n",
    "            ax.plot(b[0], b[1], marker=6, color='g')\n",
    "\n",
    "        ax.plot(self.iter, valor, marker=marker, color=color)\n",
    "\n",
    "        text1 = \"profit: {:.1f}\".format(self.profit)\n",
    "        text2 = \"   stock buying price: {:.1f}\".format(self.stock_buying_price)\n",
    "        text3 = \"   current price: {:.1f}\".format(valor)\n",
    "        text4 = \"   reward: {:.4f}\".format(self.last_reward)\n",
    "        ax.text(0.02, 0.95, text1 + text2 + text3 + text4, horizontalalignment='left', verticalalignment='center',\n",
    "                transform=ax.transAxes)\n",
    "        ax.grid()\n",
    "\n",
    "        # We additionally show a sliding window of the last 20 days to see what the agent sees.\n",
    "        ax2 = fig.add_subplot(2, 1, 2)\n",
    "        ax2.plot(range(self.iter-19, self.iter+1), data[self.index_day-21: self.index_day - 1])\n",
    "        ax2.set(xlabel='last 20 days', ylabel='price $',\n",
    "               title='20 days window')\n",
    "        ax2.plot(self.iter, valor, marker=marker, color=color)\n",
    "        for s in self.render_sell_index:\n",
    "            if self.iter - s[0] < 20:\n",
    "                ax2.plot(s[0], s[1], marker=\"v\", color='r')\n",
    "\n",
    "        for b in self.render_buy_index:\n",
    "            if self.iter - b[0] < 20:\n",
    "                ax2.plot(b[0], b[1], marker=\"^\", color='g')\n",
    "\n",
    "        ax2.grid()\n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955566cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent_discrete_parallel.Agent(actor_lr=1e-3,\n",
    "                                         critic_lr=1e-3,\n",
    "                                         batch_size=128,\n",
    "                                         memory_size=100,\n",
    "                                         epsilon=1.0,\n",
    "                                         epsilon_decay=0.97,\n",
    "                                         epsilon_min=0.15,\n",
    "                                         net_architecture=net_architecture,\n",
    "                                         n_stack=20,\n",
    "                                         loss_critic_discount=0.001,\n",
    "                                         loss_entropy_beta=0.01,\n",
    "                                         tensorboard_dir='tensorboard_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43dd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_Agent import dpg_agent\n",
    "\n",
    "net_architecture = networks.dpg_net(use_custom_network=True,\n",
    "                                    custom_network=actor_lstm_custom_model)\n",
    "agent = dpg_agent.Agent(learning_rate=1e-4,\n",
    "                        batch_size=64,\n",
    "                        net_architecture=net_architecture,\n",
    "                        n_stack=20,\n",
    "                       tensorboard_dir='tensorboard_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = rl_problem.Problem(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8930f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(1000, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ab50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Probamos el rendimiento del agente\n",
    "problem.test(render=True, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93af4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586839ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f55a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
