{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495a523",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9ae80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Custom networks with keras and Deterministic Policy Gradient (DPG)\n",
    "\n",
    "By the end of this notebook you will know how to use agents with advanced neural network using keras, this will allow you to create more complex neural architectures and use all kind of layers from keras module.\n",
    "\n",
    "We choose Deterministic Policy Gradient (DPG) for this tutorial. This is a Policy-Based agent, which means that it will learn the policy itself. Instead of proposing values of states V(s) as DQN based agent do, DPG propose directly the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4be4e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import dpg_agent\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401f4ab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining the Neural Network Architecture\n",
    "\n",
    "In file \"RL_Agent.base.utils.networks.networks\" we provide some functions to define the dictionaries with the neural network architectures. These dictionaries have a key called: \"use_custom_network\". When this key is set to True, the agent will recieves a funtion that builds a keras model. This function receives the input shape of the network which should be the state size. Inside this function you can create you keras network and return it as a tensorflow.keras.models.Sequential or as from tensorflow.keras.models.Model. \n",
    "\n",
    "The next cell shows an example of creating a Sequential keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e43f7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_custom_model(input_shape):\n",
    "    actor_model = Sequential()\n",
    "    actor_model.add(LSTM(64, input_shape=input_shape, activation='tanh'))\n",
    "    actor_model.add(Dense(256, input_shape=input_shape, activation='relu'))\n",
    "    actor_model.add(Dense(256, activation='relu'))\n",
    "    return actor_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ac607",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we need to define the dictionary to especify the network architecture. As explained before, \"use_custom_network\" parameter has to be set to True. The other parameter, \"custom_network\", recieves the function to build the model. \n",
    "\n",
    "For this particular case, we only have the \"custom_network\" param, but in other cases we may have the network divided in subnetworks. For example, Dueling DDQN or Deep Deterministic Policy Gradient have especific network architectures that are divided in subnerworks where we have an especific key in the dictionarie for each subnetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3863f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net_architecture = networks.dpg_net(use_custom_network=True,\n",
    "                                    custom_network=lstm_custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38431b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define the RL Agent\n",
    "\n",
    "Here, we define the RL agent using the next parameters:\n",
    "\n",
    "* learning_rate: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network. \n",
    "* net_architecture: net architecture defined before.\n",
    "* n_stack: number of stacked timesteps to form the state.\n",
    "\n",
    "You may notice that we do not include parameters related to exploration process like \"epsilon\", this is because this algorithm use by default a random choice of action based on the probabilities calculated by the neural network (np.random.choice(n_actions, p=probability_predicitons). This dotes DPG with an inherent explorative behavior and makes epsilon (exploration rate) not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a3b5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent = dpg_agent.Agent(learning_rate=1e-3,\n",
    "                        batch_size=64,\n",
    "                        net_architecture=net_architecture,\n",
    "                        n_stack=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d5025",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define the Environment\n",
    "\n",
    "We choose the LunarLander environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c493f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "environment = \"LunarLander-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876ea70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "Create a RL problem were the comunications between agent and environment are managed. In this case, we use the funcionality from \"RL_Problem.rl_problem.py\" which makes transparent to the user the selection of the matching problem. The function \"Problem\" automaticaly selects the problem based on the agent used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9be6eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem = rl_problem.Problem(environment, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7162846",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem that we have define. Here, we specify the number of episodes, the skip_states parameter, the render boolean and additionaly, after how many iterations we want to render the environment. \n",
    "\n",
    "When render is set to False we can specify the \"render_after\" parameter. The environement will be rendered after reach the specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb2a4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem.solve(episodes=250, skip_states=3, render=False, render_after=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51d8f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Runing the agent in exploitation mode over the environment to see the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04fc13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem.test(n_iter=4, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b722f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9bd706",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run this last cell if you want to save the agent to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd336e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent_saver.save(agent, 'agent_dpg_lunar.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d9a9d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Takeaways\n",
    "- We trained our first Policy-Based agent.\n",
    "- We learned how to use use keras for creating complex and flexibles neural network architectures within the library.\n",
    "- We learned to use a new parameter for rendering the training process after n iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad521eb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}