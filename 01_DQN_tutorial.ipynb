{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee11132",
   "metadata": {},
   "source": [
    "# Library use intruduction with Deep Q Network \n",
    "\n",
    "By the end of this notebook you will know how to implement your first reinforcement agent to solve a first standar  decission making problem.\n",
    "\n",
    "Specifically, you will learn the easiest and simplest form allowed by this library to implement a Deep Q Network (DQN) agent. This reinforcement learning (RL) agent is based on \"Q Learning\" algorithms where the known as \"Q table\" is replaced by a neural network. The decission making problem selected for this tutorial is the \"Cart Pole\" problem provided by OpenAI Gym library. Under a RL framework the implementation in a compatible format of the deccission making problem to solve is usually called the Environment.\n",
    "\n",
    "This tutorial consist mainly of four code lines where the next four task are addressed:\n",
    "\n",
    "- Define the environment.\n",
    "- Define the agent.\n",
    "- Build a RL Problem given the environment and the agent\n",
    "- Solve the RL Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daafa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import dqn_agent\n",
    "from RL_Problem.base.ValueBased import dqn_problem\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabda33",
   "metadata": {},
   "source": [
    "##  Defining the Environment\n",
    "The next cell define the environment. We use the CartPole environment of the Gym library from OpenAI. This environment consist of a cart with a pole attached and the objective is maining the pole upright moving the cart.  This is a discrete environment were the actions the agent can make are: 1) move the cart to the left or 2) move the cart to the rigth. The forces needed to move the cart are automaticatly calculated by the environment. The reward function provides a value of +1 every time step the car is upritgh. An episode ends when the pole is too far from the vertical, go outside of the screen or after 500 timesteps.\n",
    "\n",
    "Our library is compatible with all Gym environments and use a compatible interfaz based in Gym's one for defining your own envirinments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfec50",
   "metadata": {},
   "source": [
    "## Defining the RL Agent\n",
    "Here, the RL agent that is going to be used is defined. A DQN agent is used because it works very well in problems with discrete action spaces. DQN is an adaptation of Q-Learning for using neural networks and is one of the very first methods usually used to introduce the Reinforcement Learning paradigm. \n",
    "\n",
    "The agent is defined in the naivest way this library allows, where only configure a few parameters id needed. The parameters used for this example are:\n",
    "\n",
    "* learning_rate: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network. \n",
    "* epsilon: Also known as exploration rate, determines the amount of exploration (float between [0, 1]). 0 -> Full Exploitation; 1 -> Full exploration.\n",
    "* epsilon_decay: Decay factor of exploration rate. In each iteration a new epslon value is calculated as: epsilon' = epsilon * epsilon_decay.\n",
    "* esilon_min: minimun value epsilon can reach during the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dqn_agent.Agent(learning_rate=1e-3, batch_size=128, epsilon=0.4, epsilon_decay=0.999, epsilon_min=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d85953",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "In the next cell, a RL problem is created. The reinforcement learning problem is an entity were the comunications between agent and environment are managed.\n",
    "\n",
    "There are a specific problem definition for each agent that respond to the specific requeriments of each algorithm. We also provides a function that automaticaly select the correct problem given a an agent from the library. This function can be imported from \"RL_Problem.rl_problem.py\" and recieves the environment and the agent as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = dqn_problem.DQNProblem(environment, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f62c2e",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem defined above. \"solve\" function run the specified RL algorithm an receives as parameters 1) the number of iteration, which conrrespond with the number of episodes that wil be executed; 2) the \"render\" flag, wich determines if the environment will be rendered during the training process and 3) the verbosity of the function. Rendering the environment in training mode allows us to se how the agent is learning to perform better and better but some specific environment may make longer the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535319a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "problem.solve(30, render=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b68589",
   "metadata": {},
   "source": [
    "The next cell run the agent n iterations in fully explorative mode to check the performance obtained by the agent. It will be rendered by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6447040",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.test(n_iter=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ec444",
   "metadata": {},
   "source": [
    "Using \"get_histogram_metrics\" and \"plot_reward_hist\" functions the history of rewards obtained during the epochs of the training process can be visualized. Param n_moving_average select how much time steps will be used to calculate a smothed versi√≥n of the data (blue line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c817be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, n_moving_average=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccc320",
   "metadata": {},
   "source": [
    "Run this last cell if you want to save the agent to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a88089",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_saver.save(agent, 'agent_dqn_lunar.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5dff79",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "- We had the fisrt contact with the library funtionalities.\n",
    "- We learned what are the main components of a Reinforcement Learning algorithm:\n",
    "    - 1) The environment.\n",
    "    - 2) The agent.\n",
    "    - 3) The RL problem, which manages the comunication and interaction between agent and environment.\n",
    "- We learned how to define a Deep Reinforcement Learning agent in an quickly and easy way.\n",
    "- We learned how to use an evironment from OpenAI Gym library.\n",
    "- We trained and tested our firt RL agent.\n",
    "- We saw how to plot the reward history of the training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
