{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd808acf",
   "metadata": {},
   "source": [
    "# Continuous actions and Tensorboar with Deterministic Policy Gradient (DPG)\n",
    "\n",
    "In this tutorial we address for first time a problem with continuous actions. We chose the continuous version of Lunar Lander environment for this tutorial. This environment have two actions, both floats in range [-1, 1]. The first action (a_1) controls the main engine, when a_1 < 0 the engine is off and when a_1 > 1 engine is on. The second action (a_2) controls the left and rigth engines. If a_2 in [-1, -0.5] fire left engine, if a_2 in [0.5, 1] fire right engine and if a_2 in [-0.5, 0.5] engines are off.\n",
    "\n",
    "We also show how to save tensorboard summaries of the training process. We use the tensorboard funcionality defined by defaul, we will introduce how to use customized tensorboard summaries in further tutorials.\n",
    "\n",
    "By the end of this tutorial you will know how to use agents in environments with continuous action spaces and how to record Tensorboard summaries to supervise the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_Problem import rl_problem\n",
    "from RL_Agent import dpg_agent_continuous\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from RL_Agent.base.utils.networks import networks\n",
    "from RL_Agent.base.utils import agent_saver, history_utils\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb7f38",
   "metadata": {},
   "source": [
    "## Defining the  Neural Network Architecture\n",
    "We define the network architecture using the function \"dpg_net\" from \"RL_Agent.base.utils.networks.networks.py\" which return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_architecture = networks.dpg_net(dense_layers=2,\n",
    "                                    n_neurons=[128, 128],\n",
    "                                    dense_activation=['relu', 'relu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2f592",
   "metadata": {},
   "source": [
    "## Define the RL Agent\n",
    "\n",
    "We define the agent setting the next parameters:\n",
    "\n",
    "* learning_rate: learning rate for training the neural network.\n",
    "* batch_size: Size of the batches used for training the neural network.\n",
    "* net_architecture: net architecture defined before.\n",
    "* n_stack: number of stacked timesteps to form the state.\n",
    "* tensorboard_dir: path to folder for store tensorboard summaries.\n",
    "\n",
    "If we especify the \"tensorboard_dir\" param, the agent will record the default tensorboard summaries. \"tensorboard_dir\" expect a directory path in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33346559",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dpg_agent_continuous.Agent(learning_rate=1e-3,\n",
    "                            batch_size=64,\n",
    "                            n_stack=1,\n",
    "                            net_architecture=net_architecture,\n",
    "                            tensorboard_dir='tensorboard_logs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375b34e",
   "metadata": {},
   "source": [
    "## Define the Environment\n",
    "\n",
    "We chose the LunarLanderContinuous environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16020508",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"LunarLanderContinuous-v2\"\n",
    "environment = gym.make(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa267b0",
   "metadata": {},
   "source": [
    "## Build a RL Problem\n",
    "\n",
    "The RL problem is were the comunications between agent and environment are managed. In this case, we use the funcionalities from \"RL_Problem.rl_problem.py\" which makes transparent to the user the selection of the matching problem. The function \"Problem\" automaticaly selects the problem based on the used agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b36c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = rl_problem.Problem(environment, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82766658",
   "metadata": {},
   "source": [
    "## Solving the RL Problem\n",
    "\n",
    "Next step is solving the RL problem that we have define. Here, we specify the number of episodes, the skip_states parameter and additionaly after how many iterations we want to render the environment. \n",
    "\n",
    "We do not specify the value of render because it is set to False by default when trainig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.solve(episodes=400, skip_states=3, render_after=190)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3044f6e",
   "metadata": {},
   "source": [
    "Runing the agent in exploitation mode over the environment to see the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429416f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.test(n_iter=4, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd339f",
   "metadata": {},
   "source": [
    "Lets see the reward history as usual. In order to execute the next cell you will need to stop the execution of the cell avobe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0905cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = problem.get_histogram_metrics()\n",
    "history_utils.plot_reward_hist(hist, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda31e6",
   "metadata": {},
   "source": [
    "## Run Tensorboard to See the Recorded Summaries\n",
    "\n",
    "Lets see the tensorboard logs. Next cell executes the command that runs the tensorboard service. To see the result, you have to open a tab in your browser on the url that the command shows, usually http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee813319",
   "metadata": {},
   "source": [
    "Run this last cell if you want to save the agent to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99657a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_saver.save(agent, 'agent_dpg_lunar.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799bdc0",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "- We trained our agent in a environment with continuous action space.\n",
    "- We learned how to use record the default Tensorboard summaries during the training process.\n",
    "- We learned how to see the recorded summaries using the Tensorboard service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ecf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
